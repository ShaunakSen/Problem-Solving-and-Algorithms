{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## SQL Practice\n\n> Based on notes on: https://www.youtube.com/watch?v=gwp3dJUsy5g\n\n---\n\n### SQL Order of execution\n\n> https://sqlbolt.com/lesson/select_queries_order_of_execution\n\n```sql\nSELECT DISTINCT column, AGG_FUNC(column_or_expression), …\nFROM mytable\n    JOIN another_table\n      ON mytable.column = another_table.column\n    WHERE constraint_expression\n    GROUP BY column\n    HAVING constraint_expression\n    ORDER BY column ASC/DESC\n    LIMIT count OFFSET COUNT;\n```\n\n\n1. FROM and JOIN\n\nThe FROM clause, and subsequent JOINs are first executed to determine the total working set of data that is being queried. This includes subqueries in this clause, and can cause temporary tables to be created under the hood containing all the columns and rows of the tables being joined.\n\n2. WHERE\n\nOnce we have the total working set of data, the first-pass WHERE constraints are applied to the individual rows, and rows that do not satisfy the constraint are discarded. Each of the constraints can only access columns directly from the tables requested in the FROM clause. __Aliases in the SELECT part of the query are not accessible in most databases since they may include expressions dependent on parts of the query that have not yet executed.__\n\n3. GROUP BY\n\nThe remaining rows after the WHERE constraints are applied are then grouped based on common values in the column specified in the GROUP BY clause. __As a result of the grouping, there will only be as many rows as there are unique values in that column__. Implicitly, this means that you should only need to use this when you have aggregate functions in your query.\n\n4. HAVING\n\nIf the query has a GROUP BY clause, then the constraints in the HAVING clause are then applied to the grouped rows, discard the grouped rows that don't satisfy the constraint. __Like the WHERE clause, aliases are also not accessible from this step in most databases__.\n\n5. SELECT\n\nAny expressions in the SELECT part of the query are finally computed.\n\n6. DISTINCT\n\nOf the remaining rows, rows with duplicate values in the column marked as DISTINCT will be discarded.\n\n7. ORDER BY\n\nIf an order is specified by the ORDER BY clause, the rows are then sorted by the specified data in either ascending or descending order. Since all the expressions in the SELECT part of the query have been computed, you can reference aliases in this clause.\n\n\n8. LIMIT / OFFSET\n\nFinally, the rows that fall outside the range specified by the LIMIT and OFFSET are discarded, leaving the final set of rows to be returned from the query.\n\n\n\n\n\n### Difference bw DISTINCT and GROUP BY\n\n> https://www.sitepoint.com/community/t/mysql-when-group-by-is-faster-than-distinct/272420\n\nIn some cases GROUP BY might be faster than DISTICT\n\n- When you run DISTINCT MySQL has to looks across all selected columns whereas GROUP BY will only do it for whatever columns you explicitly assign to GROUP BY so there is less work to do (my query was selecting about 15 columns)\n\nthis is true as far as it goes… however, if you have GROUP BY with only a few of your 15 columns, then technically you are running an invalid query in mysql (yes, mysql will run these invalid queries), and the results are indeterminate\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00000-ee9233e5-082b-4a19-8097-b2e66ba0e976",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1401.640625
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Combining Data\n\n#### Unions\n\nPut simply, a union (SQL Union) is the process of stacking two tables on top of one another. You will usually do this when your data is split up into multiple sections like an excel spreadsheet of a year’s sales split by month.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa5adad37-92c6-4924-b52c-2ab340ae351b%2FScreen_Shot_2021-03-15_at_12.23.40_PM.png?table=block&id=199c2a41-408a-4898-a329-c7810d5d9a39&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1390&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n#### Joins\n\nJoins combine two tables horizontally. For a join, like a Union you have to have at least two tables, what we call our Left Table and our Right Table. You (mostly) have to have at least one matching column between the two tables, and you will match rows from these columns. The most common way to visualize the types of Joins are through Venn Diagrams.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fba7b4d86-db23-4142-b860-2e23fce3bae4%2FScreen_Shot_2021-03-15_at_12.36.03_PM.png?table=block&id=31a987e2-12d3-4d95-bdbc-a37449be2b29&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=2000&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe94f8697-5537-4458-9992-4ea02defbea0%2FScreen_Shot_2021-03-15_at_12.36.37_PM.png?table=block&id=91c32502-734f-4ef3-90b6-d9725da9cd3b&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=960&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Inner Join__\n\nWe’re now going to do something called an Inner Join on the [ID] column which will only output exact matches from the [ID] column in our output.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1d7856a8-7fc5-4673-a7a6-522d5fd52a9c%2FScreen_Shot_2021-03-15_at_12.37.59_PM.png?table=block&id=b5c56e80-bb05-4dc5-955b-073c3d231bfc&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1370&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Left Join__\n\nA Left Join keeps all of the data from your Left table and whatever matches from the Right table.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa4b067dc-dd79-4456-8cae-2f81a8ac4201%2FScreen_Shot_2021-03-15_at_12.39.07_PM.png?table=block&id=f0f911be-573d-415e-b829-873cd6c300c1&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1350&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Right Join__\n\nA Right Join does the exact opposite and keeps everything from your Right table while only bringing in the matches from the Left table.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1235ef85-ef65-4e03-ba26-6661b39bad62%2FScreen_Shot_2021-03-15_at_12.39.43_PM.png?table=block&id=fd1880dd-7d29-477b-b654-7e22a9b9873c&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Full Join__\n\nA Full Join brings in everything from both tables and matches whatever will match from the columns you specify.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe42b3cca-c8df-48fb-9b63-211c498be6a9%2FScreen_Shot_2021-03-15_at_12.40.52_PM.png?table=block&id=02ceefe0-20a9-48f7-831d-88698f314d43&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Cross Join__\n\nThe CROSS JOIN is used to generate a paired combination of each row of the first table with each row of the second table. This join type is also known as cartesian join.\n\nSuppose that we are sitting in a coffee shop and we decide to order breakfast. Shortly, we will look at the menu and we will start thinking of which meal and drink combination could be more tastier. Our brain will receive this signal and begin to generate all meal and drink combinations.\n\nThe following image illustrates all menu combinations that can be generated by our brain. The SQL CROSS JOIN works similarly to this mechanism, as it creates all paired combinations of the rows of the tables that will be joined.\n\n![](https://www.sqlshack.com/wp-content/uploads/2020/02/sql-cross-join-working-mechanism.png)\n\nThe SQL queries which contain the CROSS JOIN keyword can be very costly as it will need nested loops. We try to say that these queries have a high potential to consume more resources and can cause performance issues.\n\nBriefly, when we decide to use the CROSS JOIN in any query, we should consider the number of the tables that will be joined. Such as, when we CROSS JOIN two tables and if the first one contains 1000 rows and the second one contains 1000 rows, the row count of the resultset will be 1.000.000 rows.\n\n---\n\nJoins can get a bit tricky because of the potential for gotchas when joining two tables. The most common one is row duplication where you accidentally duplicate rows because the columns you’re matching on have multiple potential matches. In the example below we’re going to try an Inner Join. You’ll notice the columns in Orange were duplicated.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F89439f1d-e24a-4451-bd0d-37a0dfcc4487%2FScreen_Shot_2021-03-15_at_12.42.17_PM.png?table=block&id=9bac2f13-52ee-4ac9-9bc8-806a6d7187d0&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1370&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\nThis isn’t an error per se but it is something to watch out for as it can cause you to duplicate data you don’t intend to duplicate.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F8d64f5ba-d067-40f5-94bb-32beb9fe49d1%2FScreen_Shot_2021-03-15_at_12.42.37_PM.png?table=block&id=6a39c3e9-2900-4be6-9dc2-bbb14f68181a&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00001-5c35158c-6a2a-4e0e-95ac-2a9be709c9c7",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 6148.59375
   }
  },
  {
   "cell_type": "markdown",
   "source": "### LIKE, BETWEEN, IN\n\nhttps://www.w3schools.com/sql/sql_wildcards.asp\n\n```\nSELECT *\nFROM dataset_1\nWHERE weather LIKE 'Sun%';\n```\n\n```\nSELECT DISTINCT temperature \nFROM dataset_1\nWHERE temperature BETWEEN 29 AND 75;\n```\n\n\n```\nSELECT occupation\nFROM dataset_1\nWHERE occupation IN ('Sales & Related', 'Management');\n```\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00002-ddb51947-8335-4cf1-9437-27c50713072e",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 361.5
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Window functions\n\n> Based on tutorial : https://www.youtube.com/watch?v=Ww71knvhQ-s\n\n---\n\nWe create an `employee` table\n\n![](https://i.imgur.com/dehIA2J.png)\n\n\nwhat i want is all the cols from employee table along with a max_salary column, which displays the overall max salary\n\n```\nSELECT e.*,\nMAX(SALARY) OVER() as max_salary \nFROM employee e;\n```\n\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|max_salary|\n------+--------+---------+------+----------+\n   101|Mohan   |Admin    |  4000|     11000|\n   102|Rajkumar|HR       |  3000|     11000|\n   103|Akbar   |IT       |  4000|     11000|\n   104|Dorvin  |Finance  |  6500|     11000|\n   105|Rohit   |HR       |  3000|     11000|\n   106|Rajesh  |Finance  |  5000|     11000|\n   107|Preet   |HR       |  7000|     11000|\n   108|Maryam  |Admin    |  4000|     11000|\n   109|Sanjay  |IT       |  6500|     11000|\n   110|Vasudha |IT       |  7000|     11000|\n   111|Melinda |IT       |  8000|     11000|\n   112|Komal   |IT       | 10000|     11000|\n   113|Gautham |Admin    |  2000|     11000|\n   114|Manisha |HR       |  3000|     11000|\n   115|Chandni |IT       |  4500|     11000|\n   116|Satya   |Finance  |  6500|     11000|\n   117|Adarsh  |HR       |  3500|     11000|\n   118|Tejaswi |Finance  |  5500|     11000|\n   119|Cory    |HR       |  8000|     11000|\n   120|Monica  |Admin    |  5000|     11000|\n   121|Rosalin |IT       |  6000|     11000|\n   122|Ibrahim |IT       |  8000|     11000|\n   123|Vikram  |IT       |  8000|     11000|\n   124|Dheeraj |IT       | 11000|     11000|\n   ````\n\nSince we are using an `OVER` clause, SQL does not tream max as an agg function, it will treat it as a window function. But we have not specified any col in the over clause, so it will consider a window over the entire dataset\n\n\nNow we want the max salary for each dept along with other cols\n\n```\nSELECT e.*,\nMAX(SALARY) OVER(PARTION BY DEPT_NAME) as max_salary \nFROM employee e;\n```\n\nNow for every distinct value of dept, sql creates a window and calculates the max salary for that window\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|max_salary|\n------+--------+---------+------+----------+\n   101|Mohan   |Admin    |  4000|      5000|\n   108|Maryam  |Admin    |  4000|      5000|\n   113|Gautham |Admin    |  2000|      5000|\n   120|Monica  |Admin    |  5000|      5000|\n   104|Dorvin  |Finance  |  6500|      6500|\n   106|Rajesh  |Finance  |  5000|      6500|\n   116|Satya   |Finance  |  6500|      6500|\n   118|Tejaswi |Finance  |  5500|      6500|\n   102|Rajkumar|HR       |  3000|      8000|\n   105|Rohit   |HR       |  3000|      8000|\n   107|Preet   |HR       |  7000|      8000|\n   114|Manisha |HR       |  3000|      8000|\n   117|Adarsh  |HR       |  3500|      8000|\n   119|Cory    |HR       |  8000|      8000|\n   103|Akbar   |IT       |  4000|     11000|\n   109|Sanjay  |IT       |  6500|     11000|\n   110|Vasudha |IT       |  7000|     11000|\n   111|Melinda |IT       |  8000|     11000|\n   112|Komal   |IT       | 10000|     11000|\n   115|Chandni |IT       |  4500|     11000|\n   121|Rosalin |IT       |  6000|     11000|\n   122|Ibrahim |IT       |  8000|     11000|\n   123|Vikram  |IT       |  8000|     11000|\n   124|Dheeraj |IT       | 11000|     11000|\n```\n\nWe can use MAX, MIN, COUNT, SUM - the agg functions we use with GROUP BY\n\nBut there are some specific window functions as well:\n\n#### Row number\n\nThis simply assigns an id to every record in our table\n\n```\nSELECT e.*,\nROW_NUMBER () OVER() AS rn\nFROM employee e \n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n------+--------+---------+------+--+\n   101|Mohan   |Admin    |  4000| 1|\n   102|Rajkumar|HR       |  3000| 2|\n   103|Akbar   |IT       |  4000| 3|\n   104|Dorvin  |Finance  |  6500| 4|\n   105|Rohit   |HR       |  3000| 5|\n   106|Rajesh  |Finance  |  5000| 6|\n   107|Preet   |HR       |  7000| 7|\n   108|Maryam  |Admin    |  4000| 8|\n   109|Sanjay  |IT       |  6500| 9|\n   110|Vasudha |IT       |  7000|10|\n   111|Melinda |IT       |  8000|11|\n   112|Komal   |IT       | 10000|12|\n   113|Gautham |Admin    |  2000|13|\n   114|Manisha |HR       |  3000|14|\n   115|Chandni |IT       |  4500|15|\n   116|Satya   |Finance  |  6500|16|\n   117|Adarsh  |HR       |  3500|17|\n   118|Tejaswi |Finance  |  5500|18|\n   119|Cory    |HR       |  8000|19|\n   120|Monica  |Admin    |  5000|20|\n   121|Rosalin |IT       |  6000|21|\n   122|Ibrahim |IT       |  8000|22|\n   123|Vikram  |IT       |  8000|23|\n   124|Dheeraj |IT       | 11000|24|\n```\n\n```\nSELECT e.*,\nROW_NUMBER () OVER(PARTITION BY DEPT_NAME) AS rn\nFROM employee e \n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n------+--------+---------+------+--+\n   101|Mohan   |Admin    |  4000| 1|\n   108|Maryam  |Admin    |  4000| 2|\n   113|Gautham |Admin    |  2000| 3|\n   120|Monica  |Admin    |  5000| 4|\n   104|Dorvin  |Finance  |  6500| 1|\n   106|Rajesh  |Finance  |  5000| 2|\n   116|Satya   |Finance  |  6500| 3|\n   118|Tejaswi |Finance  |  5500| 4|\n   102|Rajkumar|HR       |  3000| 1|\n   105|Rohit   |HR       |  3000| 2|\n   107|Preet   |HR       |  7000| 3|\n   114|Manisha |HR       |  3000| 4|\n   117|Adarsh  |HR       |  3500| 5|\n   119|Cory    |HR       |  8000| 6|\n   103|Akbar   |IT       |  4000| 1|\n   109|Sanjay  |IT       |  6500| 2|\n   110|Vasudha |IT       |  7000| 3|\n   111|Melinda |IT       |  8000| 4|\n   112|Komal   |IT       | 10000| 5|\n   115|Chandni |IT       |  4500| 6|\n   121|Rosalin |IT       |  6000| 7|\n   122|Ibrahim |IT       |  8000| 8|\n   123|Vikram  |IT       |  8000| 9|\n   124|Dheeraj |IT       | 11000|10|\n```\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00003-d1aefeb0-0458-4ddc-b574-30f223486c00",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 3602.125
   }
  },
  {
   "cell_type": "markdown",
   "source": "__Say we want to fetch 1st 2 employees that joined company in each dept__\n\nAssume emp_id is lower for employees who joined earlier\n\n```\nSELECT * FROM (\n\tSELECT e.*,\n\tROW_NUMBER () OVER(PARTITION BY DEPT_NAME ORDER BY emp_ID) AS rn\n\tFROM employee e \n) x\nWHERE x.rn < 3\n```\n\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n------+--------+---------+------+--+\n   101|Mohan   |Admin    |  4000| 1|\n   108|Maryam  |Admin    |  4000| 2|\n   104|Dorvin  |Finance  |  6500| 1|\n   106|Rajesh  |Finance  |  5000| 2|\n   102|Rajkumar|HR       |  3000| 1|\n   105|Rohit   |HR       |  3000| 2|\n   103|Akbar   |IT       |  4000| 1|\n   109|Sanjay  |IT       |  6500| 2|\n```\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00004-9e02339a-11f1-44d0-81e0-b6dcc40c2785",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 475.46875
   }
  },
  {
   "cell_type": "markdown",
   "source": "__Fetch top 3 employees in each dept earning max salary__\n\nWe can use the rank or dense_rank function\n\n```\nSELECT * FROM (\n\tSELECT e.*,\n\tRANK() OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rank`\n\tFROM employee e \n) x\nWHERE x.rank < 4\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rank|\n------+--------+---------+------+----+\n   120|Monica  |Admin    |  5000|   1|\n   101|Mohan   |Admin    |  4000|   2|\n   108|Maryam  |Admin    |  4000|   2|\n   104|Dorvin  |Finance  |  6500|   1|\n   116|Satya   |Finance  |  6500|   1|\n   118|Tejaswi |Finance  |  5500|   3|\n   119|Cory    |HR       |  8000|   1|\n   107|Preet   |HR       |  7000|   2|\n   117|Adarsh  |HR       |  3500|   3|\n   124|Dheeraj |IT       | 11000|   1|\n   112|Komal   |IT       | 10000|   2|\n   111|Melinda |IT       |  8000|   3|\n   122|Ibrahim |IT       |  8000|   3|\n   123|Vikram  |IT       |  8000|   3|\n```\n\n__Rank vs Dense Rank vs Row no__\n\n```\nSELECT e.*,\nRANK() OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rank`,\nDENSE_RANK () OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `dense_rank`,\nROW_NUMBER () OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rn`\nFROM employee e \n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rank|dense_rank|rn|\n------+--------+---------+------+----+----------+--+\n   120|Monica  |Admin    |  5000|   1|         1| 1|\n   101|Mohan   |Admin    |  4000|   2|         2| 2|\n   108|Maryam  |Admin    |  4000|   2|         2| 3|\n   113|Gautham |Admin    |  2000|   4|         3| 4|\n   104|Dorvin  |Finance  |  6500|   1|         1| 1|\n   116|Satya   |Finance  |  6500|   1|         1| 2|\n   118|Tejaswi |Finance  |  5500|   3|         2| 3|\n   106|Rajesh  |Finance  |  5000|   4|         3| 4|\n   119|Cory    |HR       |  8000|   1|         1| 1|\n   107|Preet   |HR       |  7000|   2|         2| 2|\n   117|Adarsh  |HR       |  3500|   3|         3| 3|\n   102|Rajkumar|HR       |  3000|   4|         4| 4|\n   105|Rohit   |HR       |  3000|   4|         4| 5|\n   114|Manisha |HR       |  3000|   4|         4| 6|\n   124|Dheeraj |IT       | 11000|   1|         1| 1|\n   112|Komal   |IT       | 10000|   2|         2| 2|\n   111|Melinda |IT       |  8000|   3|         3| 3|\n   122|Ibrahim |IT       |  8000|   3|         3| 4|\n   123|Vikram  |IT       |  8000|   3|         3| 5|\n   110|Vasudha |IT       |  7000|   6|         4| 6|\n   109|Sanjay  |IT       |  6500|   7|         5| 7|\n   121|Rosalin |IT       |  6000|   8|         6| 8|\n   115|Chandni |IT       |  4500|   9|         7| 9|\n   103|Akbar   |IT       |  4000|  10|         8|10|\n```\n\nRank skips the next rank if there are duplicates 1->2->2->4\nDense rank does not skip ranks: 1->2->2->3\nRow number simply assigns an id to each entry, it does not care for duplicates 1->2->3->4 irrespective of duplicates\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00005-3c63e880-7211-4c8e-8014-666f6e7ab792",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1366.09375
   }
  },
  {
   "cell_type": "markdown",
   "source": "__Lead and Lag__\n\n> Also read: https://learnsql.com/blog/lead-and-lag-functions-in-sql/\n\n\nBasic Lead and Lag\n\n```\nSELECT e.*,\nLAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\nLEAD(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS next_emp_salary\nFROM employee e\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|next_emp_salary|\n------+--------+---------+------+---------------+---------------+\n   101|Mohan   |Admin    |  4000|               |           4000|\n   108|Maryam  |Admin    |  4000|           4000|           2000|\n   113|Gautham |Admin    |  2000|           4000|           5000|\n   120|Monica  |Admin    |  5000|           2000|               |\n   104|Dorvin  |Finance  |  6500|               |           5000|\n   106|Rajesh  |Finance  |  5000|           6500|           6500|\n   116|Satya   |Finance  |  6500|           5000|           5500|\n   118|Tejaswi |Finance  |  5500|           6500|               |\n   102|Rajkumar|HR       |  3000|               |           3000|\n   105|Rohit   |HR       |  3000|           3000|           7000|\n   107|Preet   |HR       |  7000|           3000|           3000|\n   114|Manisha |HR       |  3000|           7000|           3500|\n   117|Adarsh  |HR       |  3500|           3000|           8000|\n   119|Cory    |HR       |  8000|           3500|               |\n   103|Akbar   |IT       |  4000|               |           6500|\n   109|Sanjay  |IT       |  6500|           4000|           7000|\n   110|Vasudha |IT       |  7000|           6500|           8000|\n   111|Melinda |IT       |  8000|           7000|          10000|\n   112|Komal   |IT       | 10000|           8000|           4500|\n   115|Chandni |IT       |  4500|          10000|           6000|\n   121|Rosalin |IT       |  6000|           4500|           8000|\n   122|Ibrahim |IT       |  8000|           6000|           8000|\n   123|Vikram  |IT       |  8000|           8000|          11000|\n   124|Dheeraj |IT       | 11000|           8000|               |\n```\n\n\nLead and lag follow the syntax: `LAG(expression [,offset[,default_value]]) OVER(ORDER BY columns)`\n\nThese functions take three arguments: the name of the column or an expression from which the value is obtained, the number of rows to skip (offset) above, and the default value to be returned if the stored value obtained from the row above is empty. Only the first argument is required. The third argument (default value) is allowed only if you specify the second argument, the offset.\n\n```\n--- Lead and Lag\nSELECT e.*,\nLAG(SALARY, 2, -1) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\nLEAD(SALARY, 2, -1) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS next_emp_salary\nFROM employee e\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|next_emp_salary|\n------+--------+---------+------+---------------+---------------+\n   101|Mohan   |Admin    |  4000|             -1|           2000|\n   108|Maryam  |Admin    |  4000|             -1|           5000|\n   113|Gautham |Admin    |  2000|           4000|             -1|\n   120|Monica  |Admin    |  5000|           4000|             -1|\n   104|Dorvin  |Finance  |  6500|             -1|           6500|\n   106|Rajesh  |Finance  |  5000|             -1|           5500|\n   116|Satya   |Finance  |  6500|           6500|             -1|\n   118|Tejaswi |Finance  |  5500|           5000|             -1|\n   102|Rajkumar|HR       |  3000|             -1|           7000|\n   105|Rohit   |HR       |  3000|             -1|           3000|\n   107|Preet   |HR       |  7000|           3000|           3500|\n   114|Manisha |HR       |  3000|           3000|           8000|\n   117|Adarsh  |HR       |  3500|           7000|             -1|\n   119|Cory    |HR       |  8000|           3000|             -1|\n   103|Akbar   |IT       |  4000|             -1|           7000|\n   109|Sanjay  |IT       |  6500|             -1|           8000|\n   110|Vasudha |IT       |  7000|           4000|          10000|\n   111|Melinda |IT       |  8000|           6500|           4500|\n   112|Komal   |IT       | 10000|           7000|           6000|\n   115|Chandni |IT       |  4500|           8000|           8000|\n   121|Rosalin |IT       |  6000|          10000|           8000|\n   122|Ibrahim |IT       |  8000|           4500|          11000|\n   123|Vikram  |IT       |  8000|           6000|             -1|\n   124|Dheeraj |IT       | 11000|           8000|             -1|\n```\n\ncompare salary of each employee with prev one in the dept:\n\n```\nSELECT e.*,\nLAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\nCASE WHEN e.SALARY > LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'higher than prev'\n\tWHEN e.SALARY < LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'lower than prev'\n\tWHEN e.SALARY = LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'same as prev'\nEND salary_comparison\nFROM employee e\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|salary_comparison|\n------+--------+---------+------+---------------+-----------------+\n   101|Mohan   |Admin    |  4000|               |                 |\n   108|Maryam  |Admin    |  4000|           4000|     same as prev|\n   113|Gautham |Admin    |  2000|           4000|  lower than prev|\n   120|Monica  |Admin    |  5000|           2000| higher than prev|\n   104|Dorvin  |Finance  |  6500|               |                 |\n   106|Rajesh  |Finance  |  5000|           6500|  lower than prev|\n   116|Satya   |Finance  |  6500|           5000| higher than prev|\n   118|Tejaswi |Finance  |  5500|           6500|  lower than prev|\n   102|Rajkumar|HR       |  3000|               |                 |\n   105|Rohit   |HR       |  3000|           3000|     same as prev|\n   107|Preet   |HR       |  7000|           3000| higher than prev|\n   114|Manisha |HR       |  3000|           7000|  lower than prev|\n   117|Adarsh  |HR       |  3500|           3000| higher than prev|\n   119|Cory    |HR       |  8000|           3500| higher than prev|\n   103|Akbar   |IT       |  4000|               |                 |\n   109|Sanjay  |IT       |  6500|           4000| higher than prev|\n   110|Vasudha |IT       |  7000|           6500| higher than prev|\n   111|Melinda |IT       |  8000|           7000| higher than prev|\n   112|Komal   |IT       | 10000|           8000| higher than prev|\n   115|Chandni |IT       |  4500|          10000|  lower than prev|\n   121|Rosalin |IT       |  6000|           4500| higher than prev|\n   122|Ibrahim |IT       |  8000|           6000| higher than prev|\n   123|Vikram  |IT       |  8000|           8000|     same as prev|\n   124|Dheeraj |IT       | 11000|           8000| higher than prev|\n```\n",
   "metadata": {
    "tags": [],
    "cell_id": "00006-84494bf8-197c-41cf-b08a-e4906b568062",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 2367.890625
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Finding streaks\n\n> https://www.youtube.com/watch?v=VEjxlKBkZGM\n---\n\nImagine a player had the following results:\n\n![](https://i.imgur.com/ZfBBlqM.png)\n\nThe question is how do we detect these 2 streaks\n\nThe method we use here is\n\n- Add row no for wins and losses\n- Add row no for wins separately and loss separately\n- subtract the 2\n\n![](https://i.imgur.com/HhXfnny.png)\n\nThe subtraction gives us a column like `streak_id`\n\nWe do this in the following CTE:\n\n```sql\nWITH CTE AS\n(\n    SELECT *,\n    ROW_NUMBER() OVER(PARTITION BY player_id ORDER BY match_date) AS `all_id`,\n    ROW_NUMBER() OVER(PARTITION BY player_id, match_result ORDER BY match_date) AS `win_loss_id`\n    FROM\n    players_results\n    ORDER BY player_id, match_date\n)\n\n```\n\n\nThis gives us result like:\n\n![](https://i.imgur.com/p3f5y35.png)\n\nNow we compute the `streak_id`\n\n```sql\nCTE2 AS\n(\nSELECT \nplayer_id,\nmatch_date,\nmatch_result,\nCAST(win_loss_id AS SIGNED) - CAST(all_id AS SIGNED) AS `streak_id`\nFROM CTE\n)\n```\n\nThis gives us:\n\n![](https://i.imgur.com/6AY21Nd.png)\n\nNow for each streak_id we want to count no of times it has occured (note streak ids might get repeated for wins and losses and also for diff players)\nNow we only care about winning results, so we filter by wins\n\n```sql\nCTE3 AS(\nSELECT player_id,\nstreak_id, COUNT(*) AS `win_streak_length`\nFROM CTE2\nWHERE match_result='W'\nGROUP BY player_id, streak_id\n)\n```\n\n![](https://i.imgur.com/ab5PQP7.png)\n\nFinally we can get the max of winning streak lengths per player id:\n\n```sql\nSELECT player_id, \nMAX(win_streak_length)\nFROM CTE3\nGROUP BY player_id\n```\n\n![](https://i.imgur.com/qGxVHje.png)\n",
   "metadata": {
    "cell_id": "c332944a-dffa-4476-aa71-1f5aeb738e1b",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 4382.046875
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### SQL Question 1\n\n> https://platform.stratascratch.com/coding/9899-percentage-of-total-spend?python&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n\n```\n--- INNER JOIN AS WE WANT ONLY CUSTOMERS WHO HAVE PLACED AN ORDER\n\nselect first_name,order_details,\ntotal_order_cost/SUM(total_order_cost) OVER(PARTITION BY first_name) AS \"percentage of the order cost\"\n\nfrom orders o\nINNER JOIN customers c\nON o.cust_id = c.id\n```\n\n#### SQL Question 2\n\n> https://platform.stratascratch.com/coding/2036-lowest-revenue-generated-restaurants?python&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n\n```\n--- Filter data to only use May 2020 records\nSELECT * FROM (\nSELECT \n    restaurant_id,\n    order_total,\n    NTILE(100) OVER (ORDER BY order_total ASC) AS percentile_value\nFROM (\n\nSELECT \n    --EXTRACT(MONTH FROM customer_placed_order_datetime) as order_month,\n    --EXTRACT(YEAR FROM customer_placed_order_datetime) as order_year,\n    restaurant_id, \n    SUM(order_total) AS order_total\nFROM doordash_delivery\nWHERE EXTRACT(MONTH FROM customer_placed_order_datetime) = 5 and EXTRACT(YEAR FROM customer_placed_order_datetime) = 2020\nGROUP BY restaurant_id\n) as sub1\n) as sub0\nWHERE percentile_value < 3\nORDER BY 2 ASC\n```\n\nInstead of a subquery we can use a CTE, which is a bit easier to interpret\nAlse CTE creates temp tables, so we can re-use the query later\n",
   "metadata": {
    "tags": [],
    "cell_id": "00007-1cbcb41f-c94e-4d5d-97b7-5f78cfddcaaf",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 861.78125
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Leetcode 180. Consecutive Numbers\n\n> https://leetcode.com/problems/consecutive-numbers/\n\n```sql\nWITH CTE AS (\nSELECT\nnum,\nLAG(num, 1) OVER() AS `prev_1`,\nLAG(num, 2) OVER() AS `prev_2`\nFROM \nLogs\n),\nCTE2 AS (\n    SELECT\n    CASE WHEN num = prev_1 and num = prev_2 THEN num\n    ELSE NULL END AS `ConsecutiveNums`\n    FROM CTE\n)\n\nSELECT DISTINCT ConsecutiveNums FROM CTE2\nWHERE ConsecutiveNums IS NOT NULL\n```",
   "metadata": {
    "tags": [],
    "cell_id": "00008-33bfcba5-46be-40a8-bb9b-b069c2c878fa",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 522.921875
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Leetcode 262. Trips and Users\n\n> https://leetcode.com/problems/trips-and-users/\n\n```sql\n\n# Write your MySQL query statement below\nWITH CTE AS (\nSELECT request_at, status, u.banned AS user_banned, u1.banned AS driver_banned\nFROM Trips t\nINNER JOIN Users u\nON t.client_id = u.users_id\nINNER JOIN Users u1 \nON t.driver_id = u1.users_id\n    \nWHERE u.banned = \"No\" and u1.banned = \"No\" and request_at BETWEEN \"2013-10-01\" and \"2013-10-03\"\n),\n\nCTE2 AS\n(\nSELECT request_at,\nCASE WHEN status LIKE \"cancelled%\" THEN 1\nELSE 0 END AS new_status\nFROM CTE\n),\nCTE3 AS \n(\nSELECT \n    request_at AS Day,\n    ROUND(SUM(new_status)/COUNT(new_status), 2) AS `Cancellation Rate`\n    FROM CTE2\n    GROUP BY request_at\n)\n\nSELECT * FROM CTE3\n\n```",
   "metadata": {
    "tags": [],
    "cell_id": "00009-1686b787-49d0-4892-a03e-1be85c32029d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 813.9375
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Question: Monthly Percentage Difference\n\n> https://platform.stratascratch.com/coding/10319-monthly-percentage-difference?python=\n\n---\n\n### Soln\n\n```sql\n## get SUM(revenue) per year, month\nWITH CTE AS (\nselect YEAR(created_at) AS `year`, MONTH(created_at) AS `month`, SUM(value) AS `monthly_revenue`\nfrom sf_transactions\nGROUP BY year, month\nORDER BY year, month\n),\n\n### get prev month revenue\nCTE2 AS (\nSELECT year, month, monthly_revenue,\nLAG(monthly_revenue) OVER() AS `prev_revenue`\nFROM CTE\n),\n\n### get % change in revenue\nCTE3 AS (\nSELECT year, month, monthly_revenue,prev_revenue,\nROUND((monthly_revenue-prev_revenue)*100/prev_revenue,2) AS `perc_change`\nFROM CTE2\n),\n\n### Zero pad month\nCTE4 AS (\nSELECT \nyear, month, monthly_revenue,prev_revenue,perc_change,\nCASE WHEN month < 10 THEN CONCAT(\"0\", month)\nELSE month \nEND AS `zp_month`\nFROM CTE3\n)\n\n### final result\nSELECT\nCONCAT(year, \"-\", zp_month),\nperc_change\nFROM CTE4\n```",
   "metadata": {
    "cell_id": "0ea44f6b-e8d4-49f2-8459-5ba115252950",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1019.625
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Activity Rank\n\n> https://platform.stratascratch.com/coding/10351-activity-rank?python=\n---\n\n\n```sql\n### cte to compute num of emails sent by each user\n### and order by reqd specifications\n### cannot use rank() as duplicates should have diff ranks\nWITH CTE AS \n(\nselect from_user, COUNT(from_user) AS `total_sent`\nFROM google_gmail_emails\nGROUP BY from_user\nORDER BY COUNT(from_user) DESC, from_user ASC\n)\n\nSELECT from_user, total_sent,\nROW_NUMBER() OVER() AS `custom_rank`\nFROM CTE\n```",
   "metadata": {
    "cell_id": "d4d56c58-bb61-4a35-b78c-e14a22934138",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 456.75
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Popularity Percentage\n\n> https://platform.stratascratch.com/coding/10284-popularity-percentage?python=&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n\n> https://www.youtube.com/watch?v=_gy1o9UH2dQ\n\n---\n\n### Soln\n\n```sql\n### get each user1 and count(user2) \n### get each user2 and count(user1)\n### UNION these to get each user and total_friends\n\nWITH CTE AS (\nselect user1, COUNT(user2) AS `total_friends`\nFROM facebook_friends\nGROUP BY user1\n\nUNION\n\nselect user2, COUNT(user1) AS `total_friends`\nFROM facebook_friends\nGROUP BY user2\n),\n\n### get each user and take a sum of total_friends in case of repeated user ids\n\nCTE2 AS (\nSELECT user1 AS user, \nSUM(total_friends) AS `total_friends`\nFROM CTE\nGROUP BY user1\n)\n\n### compute % - get total no of users in the table from a subquery\n\nSELECT user,\ntotal_friends*100/(SELECT COUNT(*) FROM CTE2)\nFROM CTE2\n\nORDER BY user\n\n```",
   "metadata": {
    "cell_id": "17a08a47-073e-41f6-9915-880d0fb6994c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 944.078125
   }
  },
  {
   "cell_type": "markdown",
   "source": "## SQL Moving Averages\n\n> https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/\n\n> https://learnsql.com/blog/moving-average-in-sql/\n---\n\nThe moving average is a time series technique for analyzing and determining trends in data. Sometimes called rolling means, rolling averages, or running averages, they are calculated as the mean of the current and a specified number of immediately preceding values for each point in time. The main idea is to examine how these averages behave over time instead of examining the behavior of the original or raw data points.\n\n![](https://learnsql.com/blog/moving-average-in-sql/Image-2.png)\n\n```sql\nselect *,\n  avg(Price) OVER(ORDER BY Date\n     ROWS BETWEEN 2 PRECEDING AND CURRENT ROW )\n     as moving_average\nfrom stock_price;\n```\n\n- We use a window function, denoted with an OVER clause. As explained earlier, the rows are not collapsed, and each row has its own window over which a calculation is performed.\n\n- The size of the window in our example is 3. For each given row, we take the row itself and the two previous rows, and we calculate the average price from those three rows. This is denoted by the `ROW` keyword in the statement: `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW`. This statement says that, for each row in the table, something is calculated as an aggregation of the current and the previous two rows. This means that the moving average for each row is calculated as the mean price from the given day and the two previous days.\n\n- We have a different window frame for each day. Below, you can see an illustration of the window frame used for the row corresponding to January 9 (in green) and the window frame used for the row corresponding to June 27 (in blue):\n\n![](https://learnsql.com/blog/moving-average-in-sql/Image-5.png)\n\n- It is important that the data __not have any gaps in dates__. For each day, we need to calculate the average of the prices from that day and the two previous days. If there are missing dates in the data, this analysis will not make sense.\n\n- The `ORDER BY` keyword inside the `OVER` clause defines the order of the rows over which the moving average should be calculated. In our example, the rows are first sorted by the date column, then the window frame is defined, and the calculation is performed.\n\n- For this example, we do not use the `PARTITION BY` keyword in the `OVER` clause. `PARTITION BY` groups rows into logical chunks by some category, but we are not grouping rows that way here. In effect, our whole data set is just one large partition. \n\n\n```sql\n\nselect *,\n  avg(Price) OVER(ORDER BY Date\n      ROWS BETWEEN 1 PRECEDING AND CURRENT ROW )\n     as 2day_moving_average,\n  avg(Price) OVER(ORDER BY Date\n      ROWS BETWEEN 29 PRECEDING AND CURRENT ROW )\n      as 30day_moving_average\nfrom stock_price;\n\n```\n\n![](https://learnsql.com/blog/moving-average-in-sql/3.png)\n\nNow let's practice a little more to recap what we have learned so far. COVID-19 is still very real in our lives, so we will calculate the seven-day moving average for the total number of confirmed cases by country. The number of confirmed cases for each day for each country is stored in a table named confirmed_covid, in the column confirmed_day:\n\n```sql\nSELECT *,\n      avg(confirmed_day) OVER(\n          PARTITION BY country\n          ORDER BY date\n          ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n          AS 7day_moving_average\nFROM confirmed_covid;\n```\n\nYou may have noticed this code looks just like the one we wrote for the stock price moving average. The one key difference is that we have partitions here?—?each country is a partition, because we are calculating the seven-day averages separately for each country. Otherwise, only the table and the column names are different. You can easily adapt this code to any other business scenario.\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00008-3a7f2aa0-b931-481b-93f6-0d3b646e8487",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 3177.625
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Meta SQL question \n\n> https://towardsdatascience.com/the-facebook-data-scientist-interview-38556739e872\n---\n\nGiven two tables. One an attendance log for every student in a school district and the other a summary table with demographics for each student in the district.\n\n```\nattendance_events : date | student_id | attendance\n\nall_students : student_id | school_id | grade_level | date_of_birth | hometown\n```\n\n__What percent of students attend school on their birthday?__\n\n\n```sql\n-- step 1: join the 2 tables\nWITH cte AS\n(\n\tSELECT date, a.student_id, attendance, date_of_birth \n\t\tFROM attendance a\n\t\tINNER JOIN students s\n\t\tON a.student_id = b.student_id\n),\n-- step2: get num students attending on bday\ncte2 AS (\n\tSELECT COUNT(DISTINCT student_id) AS num_students_bday\n  FROM cte \n\tWHERE date = date_of_birth\n),\n\n--- ratio = num students / students attending on bday\nSELECT (SELECT num_students_bday FROM cte2)*100/COUNT(DISTINCT student_id) AS perc_bday\nFROM cte\n```\n\n__Which grade level had the largest drop in attendance between yesterday and today?__\n\n```sql\n-- step 1: join the 2 tables and compute the total attendance for each day and grade\nWITH cte AS\n(\n\tSELECT date, grade_level, SUM(attendance) AS attendance \n\tFROM attendance a\n\tINNER JOIN students s\n\tON a.student_id = b.student_id\n\tGROUP BY date, grade_level\n),\n-- step 2: get prev day attendance by grade level ordered by date\ncte2 AS\n(\n\tSELECT date, grade_level, attendance,\n\tLAG(attendance) OVER (PARTITION BY grade_level ORDER BY date) AS prev_day_attendance\n\tFROM cte\n),\n-- step 3: compute attendance drop for current day\ncte3 AS\n(\n\tSELECT date, grade_level, attendance, prev_day_attendance,\n\tprev_day_attendance - attendance AS attendance_drop\n\tFROM cte2\n\tWHERE date = (SELECT CURDATE())\n),\n-- step 4: get grade level with max attendance drop\ncte4 AS \n(\n\tSELECT grade_level\n\tFROM cte3\n\tWHERE attendance_drop = (SELECT MAX(attendance_drop) FROM cte3)\n)\n```\n",
   "metadata": {
    "cell_id": "5ca3221a-a0a7-4fd1-812b-19d9646fb622",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1443.828125
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Goodwater Analytics Question\n\n> https://www.youtube.com/watch?v=FC5X09o85xE\n\n---\n\n![](https://i.imgur.com/oSsFImr.png)\n\n\n![](https://i.imgur.com/JkjgDUx.png)",
   "metadata": {
    "cell_id": "ab282c7c-6dff-4ad2-9719-d7d4e1e03611",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 916.3125
   }
  },
  {
   "cell_type": "markdown",
   "source": "```sql\n--- get all downloads from ios and android and UNION them\n\nWITH all_product_downloads AS\n(\n    SELECT product_id, ios_app_id AS app_id, date, downloads\n    FROM Products p\n    INNER JOIN Downloads d\n    ON p.ios_app_id = d.app_id\n\n    UNION\n\n    SELECT product_id, android_app_id AS app_id, date, downloads\n    FROM Products p\n    INNER JOIN Downloads d\n    ON p.android_app_id = d.app_id\n),\n--- aggregate downloads at a product - date level\nproduct_date_data AS (\n    SELECT product_id, date, SUM(downloads) AS total_downloads\n    FROM all_product_downloads\n    GROUP BY product_id, date\n\n)\n```\n\nNow we have product - date level downloads data\n\nImagine at the next step we calculate 28_day_rolling_downloads like:\n\n`product_id | date | 28_day_rolling_avg`\n\n```sql\n28_day_rolling_downloads AS\n(\nSELECT product_id, date,\nAVG(downloads) OVER(PARTITION BY product_id ORDER BY date\nROWS BETWEEN 27 PRECEEDING AND CURRENT ROW\n) AS 28_day_rolling_avg\nFROM product_date_data\n\nORDER BY product, date\n)\n```\n\n```sql\n\n--- get prev year rolling avg data\n\nyoy_data AS (\nSELECT product_id, date, 28_day_rolling_avg\nLAG(28_day_rolling_avg, 365) OVER(PARTITION BY product_id ORDER BY date) AS prev_year_avg\nFROM rolling_cte\n)\n\nSELECT product, date,\n(28_day_rolling_avg - prev_year_avg)*100/prev_year_avg as YoY_growth\nFROM rolling_cte\n```\n",
   "metadata": {
    "cell_id": "252c597a-e42d-4398-8698-785fe657a4b3",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 1206.53125
   }
  },
  {
   "cell_type": "markdown",
   "source": "Alternative: Imagine u cant compute rolling avg directly - how will u do it\n\nproduct_id | date | total_downloads\n\nThe way to do this is by using a self join\nWe can specify AND conditions while joining which tells sql\nto keep rows where the datediff is bw 0 and 28\n\n```sql\n\n28_day_rolling_downloads AS(\nSELECT\np1.product_id, p1.date, AVG(p2.total_downloads) AS 28_day_rolling_avg\nFROM product_date_data p1\nINNER JOIN product_date_data p2\nON p1.product_id = p2.product_id\nAND DATEDIFF(p1.date, p2.date) <= 28\nAND DATEDIFF(p1.date, p2.date) >= 0\n\nGROUP BY p1.product_id, p1.date\n)\n```\n\n\nSELECT d1.product_id, d1.date, (d2.28_day_rolling_avg\nFROM 28_day_rolling_downloads d1\nINNER JOIN 28_day_rolling_downloads d2\nON d1.product_id = d2.product_id\nAND DATEDIFF(d1.date, d2.date) = 365\n",
   "metadata": {
    "cell_id": "7b705951-93cf-427a-b11d-35754fe57417",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 472.578125
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Advanced Facebook Data Science SQL interview question\n\n> https://www.youtube.com/watch?v=PlpUo6bHsBQ\n\n#### Soln:\n\n```sql\n-- join the two tables and filter on dates\nWITH cte AS\n(\nselect a.user_id, created_at, country, number_of_comments, MONTH(created_at) AS month\nfrom fb_comments_count a\nINNER JOIN fb_active_users b\nON a.user_id = b.user_id\nWHERE created_at >= '2019-12-01' AND created_at <= '2020-01-31'\n),\n-- country - total comments for dec\ndec_data AS\n(\nSELECT \ncountry, SUM(number_of_comments) AS total_comments\nFROM cte\nWHERE month = 12\nGROUP BY country\n),\n-- country - total comments for jan\njan_data AS \n(\nSELECT \ncountry, SUM(number_of_comments) AS total_comments\nFROM cte\nWHERE month = 1\nGROUP BY country\n),\n-- country rank for dec\ndec_rank AS\n(\nSELECT country,total_comments,\nDENSE_RANK() OVER(ORDER BY total_comments DESC) AS `rank`\nFROM dec_data\n),\n-- country rank for jan\njan_rank AS\n(\nSELECT country,total_comments,\nDENSE_RANK() OVER(ORDER BY total_comments DESC) AS `rank`\nFROM jan_data\n)\n-- join the two\nSELECT a.country, a.rank AS dec_rank, b.rank AS jan_rank\nFROM dec_rank a\nINNER JOIN jan_rank b\nON a.country = b.country\n\n```",
   "metadata": {
    "cell_id": "b9623962771a4f04934a987c9ccc9c3e",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 330.3125
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "fd6cbfcb-9dd6-4043-8df5-7e97f309b394",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 45.953125
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "cell_id": "1264a79a-f659-4191-9683-adb948932bd2",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 45.953125
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c9f7b205-46e2-4f7d-8027-1722d788f5d8' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "3f480396-3090-4c9d-95d9-b94e0489f5a1",
  "deepnote_execution_queue": []
 }
}