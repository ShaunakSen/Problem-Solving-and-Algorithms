{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## SQL Practice\n\n> Based on notes on: https://www.youtube.com/watch?v=gwp3dJUsy5g\n\n---\n\n### SQL Order of execution\n\n> https://sqlbolt.com/lesson/select_queries_order_of_execution\n\n```sql\nSELECT DISTINCT column, AGG_FUNC(column_or_expression), …\nFROM mytable\n    JOIN another_table\n      ON mytable.column = another_table.column\n    WHERE constraint_expression\n    GROUP BY column\n    HAVING constraint_expression\n    ORDER BY column ASC/DESC\n    LIMIT count OFFSET COUNT;\n```\n\n\n1. FROM and JOIN\n\nThe FROM clause, and subsequent JOINs are first executed to determine the total working set of data that is being queried. This includes subqueries in this clause, and can cause temporary tables to be created under the hood containing all the columns and rows of the tables being joined.\n\n2. WHERE\n\nOnce we have the total working set of data, the first-pass WHERE constraints are applied to the individual rows, and rows that do not satisfy the constraint are discarded. Each of the constraints can only access columns directly from the tables requested in the FROM clause. __Aliases in the SELECT part of the query are not accessible in most databases since they may include expressions dependent on parts of the query that have not yet executed.__\n\n3. GROUP BY\n\nThe remaining rows after the WHERE constraints are applied are then grouped based on common values in the column specified in the GROUP BY clause. __As a result of the grouping, there will only be as many rows as there are unique values in that column__. Implicitly, this means that you should only need to use this when you have aggregate functions in your query.\n\n4. HAVING\n\nIf the query has a GROUP BY clause, then the constraints in the HAVING clause are then applied to the grouped rows, discard the grouped rows that don't satisfy the constraint. __Like the WHERE clause, aliases are also not accessible from this step in most databases__.\n\n5. SELECT\n\nAny expressions in the SELECT part of the query are finally computed.\n\n6. DISTINCT\n\nOf the remaining rows, rows with duplicate values in the column marked as DISTINCT will be discarded.\n\n7. ORDER BY\n\nIf an order is specified by the ORDER BY clause, the rows are then sorted by the specified data in either ascending or descending order. Since all the expressions in the SELECT part of the query have been computed, you can reference aliases in this clause.\n\n\n8. LIMIT / OFFSET\n\nFinally, the rows that fall outside the range specified by the LIMIT and OFFSET are discarded, leaving the final set of rows to be returned from the query.\n\n\n\n\n\n### Difference bw DISTINCT and GROUP BY\n\n> https://www.sitepoint.com/community/t/mysql-when-group-by-is-faster-than-distinct/272420\n\nIn some cases GROUP BY might be faster than DISTICT\n\n- When you run DISTINCT MySQL has to looks across all selected columns whereas GROUP BY will only do it for whatever columns you explicitly assign to GROUP BY so there is less work to do (my query was selecting about 15 columns)\n\nthis is true as far as it goes… however, if you have GROUP BY with only a few of your 15 columns, then technically you are running an invalid query in mysql (yes, mysql will run these invalid queries), and the results are indeterminate\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00000-ee9233e5-082b-4a19-8097-b2e66ba0e976",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Combining Data\n\n#### Unions\n\nPut simply, a union (SQL Union) is the process of stacking two tables on top of one another. You will usually do this when your data is split up into multiple sections like an excel spreadsheet of a year’s sales split by month.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa5adad37-92c6-4924-b52c-2ab340ae351b%2FScreen_Shot_2021-03-15_at_12.23.40_PM.png?table=block&id=199c2a41-408a-4898-a329-c7810d5d9a39&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1390&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n#### Joins\n\nJoins combine two tables horizontally. For a join, like a Union you have to have at least two tables, what we call our Left Table and our Right Table. You (mostly) have to have at least one matching column between the two tables, and you will match rows from these columns. The most common way to visualize the types of Joins are through Venn Diagrams.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fba7b4d86-db23-4142-b860-2e23fce3bae4%2FScreen_Shot_2021-03-15_at_12.36.03_PM.png?table=block&id=31a987e2-12d3-4d95-bdbc-a37449be2b29&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=2000&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe94f8697-5537-4458-9992-4ea02defbea0%2FScreen_Shot_2021-03-15_at_12.36.37_PM.png?table=block&id=91c32502-734f-4ef3-90b6-d9725da9cd3b&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=960&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Inner Join__\n\nWe’re now going to do something called an Inner Join on the [ID] column which will only output exact matches from the [ID] column in our output.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1d7856a8-7fc5-4673-a7a6-522d5fd52a9c%2FScreen_Shot_2021-03-15_at_12.37.59_PM.png?table=block&id=b5c56e80-bb05-4dc5-955b-073c3d231bfc&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1370&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Left Join__\n\nA Left Join keeps all of the data from your Left table and whatever matches from the Right table.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa4b067dc-dd79-4456-8cae-2f81a8ac4201%2FScreen_Shot_2021-03-15_at_12.39.07_PM.png?table=block&id=f0f911be-573d-415e-b829-873cd6c300c1&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1350&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Right Join__\n\nA Right Join does the exact opposite and keeps everything from your Right table while only bringing in the matches from the Left table.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1235ef85-ef65-4e03-ba26-6661b39bad62%2FScreen_Shot_2021-03-15_at_12.39.43_PM.png?table=block&id=fd1880dd-7d29-477b-b654-7e22a9b9873c&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Full Join__\n\nA Full Join brings in everything from both tables and matches whatever will match from the columns you specify.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe42b3cca-c8df-48fb-9b63-211c498be6a9%2FScreen_Shot_2021-03-15_at_12.40.52_PM.png?table=block&id=02ceefe0-20a9-48f7-831d-88698f314d43&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n__Cross Join__\n\nThe CROSS JOIN is used to generate a paired combination of each row of the first table with each row of the second table. This join type is also known as cartesian join.\n\nSuppose that we are sitting in a coffee shop and we decide to order breakfast. Shortly, we will look at the menu and we will start thinking of which meal and drink combination could be more tastier. Our brain will receive this signal and begin to generate all meal and drink combinations.\n\nThe following image illustrates all menu combinations that can be generated by our brain. The SQL CROSS JOIN works similarly to this mechanism, as it creates all paired combinations of the rows of the tables that will be joined.\n\n![](https://www.sqlshack.com/wp-content/uploads/2020/02/sql-cross-join-working-mechanism.png)\n\nThe SQL queries which contain the CROSS JOIN keyword can be very costly as it will need nested loops. We try to say that these queries have a high potential to consume more resources and can cause performance issues.\n\nBriefly, when we decide to use the CROSS JOIN in any query, we should consider the number of the tables that will be joined. Such as, when we CROSS JOIN two tables and if the first one contains 1000 rows and the second one contains 1000 rows, the row count of the resultset will be 1.000.000 rows.\n\n---\n\nJoins can get a bit tricky because of the potential for gotchas when joining two tables. The most common one is row duplication where you accidentally duplicate rows because the columns you’re matching on have multiple potential matches. In the example below we’re going to try an Inner Join. You’ll notice the columns in Orange were duplicated.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F89439f1d-e24a-4451-bd0d-37a0dfcc4487%2FScreen_Shot_2021-03-15_at_12.42.17_PM.png?table=block&id=9bac2f13-52ee-4ac9-9bc8-806a6d7187d0&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1370&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\nThis isn’t an error per se but it is something to watch out for as it can cause you to duplicate data you don’t intend to duplicate.\n\n![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F8d64f5ba-d067-40f5-94bb-32beb9fe49d1%2FScreen_Shot_2021-03-15_at_12.42.37_PM.png?table=block&id=6a39c3e9-2900-4be6-9dc2-bbb14f68181a&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00001-5c35158c-6a2a-4e0e-95ac-2a9be709c9c7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### LIKE, BETWEEN, IN\n\nhttps://www.w3schools.com/sql/sql_wildcards.asp\n\n```\nSELECT *\nFROM dataset_1\nWHERE weather LIKE 'Sun%';\n```\n\n```\nSELECT DISTINCT temperature \nFROM dataset_1\nWHERE temperature BETWEEN 29 AND 75;\n```\n\n\n```\nSELECT occupation\nFROM dataset_1\nWHERE occupation IN ('Sales & Related', 'Management');\n```\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00002-ddb51947-8335-4cf1-9437-27c50713072e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Window functions\n\n> Based on tutorial : https://www.youtube.com/watch?v=Ww71knvhQ-s\n\n---\n\nWe create an `employee` table\n\n![](https://i.imgur.com/dehIA2J.png)\n\n```\nSELECT e.*,\nMAX(SALARY) OVER() as max_salary \nFROM employee e;\n```\n\nThis gives us max salary for each dept\n\n```\nDEPT_NAME|MAX(SALARY)|\n---------+-----------+\nAdmin    |       5000|\nFinance  |       6500|\nHR       |       8000|\nIT       |      11000|\n```\n\nwhat i want is all the cols from employee table along with a max_salary column, which displays the overall max salary\n\n```\nSELECT e.*,\nMAX(SALARY) OVER() as max_salary \nFROM employee e;\n```\n\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|max_salary|\n------+--------+---------+------+----------+\n   101|Mohan   |Admin    |  4000|     11000|\n   102|Rajkumar|HR       |  3000|     11000|\n   103|Akbar   |IT       |  4000|     11000|\n   104|Dorvin  |Finance  |  6500|     11000|\n   105|Rohit   |HR       |  3000|     11000|\n   106|Rajesh  |Finance  |  5000|     11000|\n   107|Preet   |HR       |  7000|     11000|\n   108|Maryam  |Admin    |  4000|     11000|\n   109|Sanjay  |IT       |  6500|     11000|\n   110|Vasudha |IT       |  7000|     11000|\n   111|Melinda |IT       |  8000|     11000|\n   112|Komal   |IT       | 10000|     11000|\n   113|Gautham |Admin    |  2000|     11000|\n   114|Manisha |HR       |  3000|     11000|\n   115|Chandni |IT       |  4500|     11000|\n   116|Satya   |Finance  |  6500|     11000|\n   117|Adarsh  |HR       |  3500|     11000|\n   118|Tejaswi |Finance  |  5500|     11000|\n   119|Cory    |HR       |  8000|     11000|\n   120|Monica  |Admin    |  5000|     11000|\n   121|Rosalin |IT       |  6000|     11000|\n   122|Ibrahim |IT       |  8000|     11000|\n   123|Vikram  |IT       |  8000|     11000|\n   124|Dheeraj |IT       | 11000|     11000|\n   ````\n\nSince we are using an `OVER` clause, SQL does not tream max as an agg function, it will treat it as a window function. But we have not specified any col in the over clause, so it will consider a window over the entire dataset\n\n\nNow we want the max salary for each dept along with other cols\n\n```\nSELECT e.*,\nMAX(SALARY) OVER() as max_salary \nFROM employee e;\n```\n\nNow for every distinct value of dept, sql creates a window and calculates the max salary for that window\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|max_salary|\n------+--------+---------+------+----------+\n   101|Mohan   |Admin    |  4000|      5000|\n   108|Maryam  |Admin    |  4000|      5000|\n   113|Gautham |Admin    |  2000|      5000|\n   120|Monica  |Admin    |  5000|      5000|\n   104|Dorvin  |Finance  |  6500|      6500|\n   106|Rajesh  |Finance  |  5000|      6500|\n   116|Satya   |Finance  |  6500|      6500|\n   118|Tejaswi |Finance  |  5500|      6500|\n   102|Rajkumar|HR       |  3000|      8000|\n   105|Rohit   |HR       |  3000|      8000|\n   107|Preet   |HR       |  7000|      8000|\n   114|Manisha |HR       |  3000|      8000|\n   117|Adarsh  |HR       |  3500|      8000|\n   119|Cory    |HR       |  8000|      8000|\n   103|Akbar   |IT       |  4000|     11000|\n   109|Sanjay  |IT       |  6500|     11000|\n   110|Vasudha |IT       |  7000|     11000|\n   111|Melinda |IT       |  8000|     11000|\n   112|Komal   |IT       | 10000|     11000|\n   115|Chandni |IT       |  4500|     11000|\n   121|Rosalin |IT       |  6000|     11000|\n   122|Ibrahim |IT       |  8000|     11000|\n   123|Vikram  |IT       |  8000|     11000|\n   124|Dheeraj |IT       | 11000|     11000|\n```\n\nWe can use MAX, MIN, COUNT, SUM - the agg functions we use with GROUP BY\n\nBut there are some specific window functions as well:\n\n#### Row number\n\nThis simply assigns an id to every record in our table\n\n```\nSELECT e.*,\nROW_NUMBER () OVER() AS rn\nFROM employee e \n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n------+--------+---------+------+--+\n   101|Mohan   |Admin    |  4000| 1|\n   102|Rajkumar|HR       |  3000| 2|\n   103|Akbar   |IT       |  4000| 3|\n   104|Dorvin  |Finance  |  6500| 4|\n   105|Rohit   |HR       |  3000| 5|\n   106|Rajesh  |Finance  |  5000| 6|\n   107|Preet   |HR       |  7000| 7|\n   108|Maryam  |Admin    |  4000| 8|\n   109|Sanjay  |IT       |  6500| 9|\n   110|Vasudha |IT       |  7000|10|\n   111|Melinda |IT       |  8000|11|\n   112|Komal   |IT       | 10000|12|\n   113|Gautham |Admin    |  2000|13|\n   114|Manisha |HR       |  3000|14|\n   115|Chandni |IT       |  4500|15|\n   116|Satya   |Finance  |  6500|16|\n   117|Adarsh  |HR       |  3500|17|\n   118|Tejaswi |Finance  |  5500|18|\n   119|Cory    |HR       |  8000|19|\n   120|Monica  |Admin    |  5000|20|\n   121|Rosalin |IT       |  6000|21|\n   122|Ibrahim |IT       |  8000|22|\n   123|Vikram  |IT       |  8000|23|\n   124|Dheeraj |IT       | 11000|24|\n```\n\n```\nSELECT e.*,\nROW_NUMBER () OVER(PARTITION BY DEPT_NAME) AS rn\nFROM employee e \n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n------+--------+---------+------+--+\n   101|Mohan   |Admin    |  4000| 1|\n   108|Maryam  |Admin    |  4000| 2|\n   113|Gautham |Admin    |  2000| 3|\n   120|Monica  |Admin    |  5000| 4|\n   104|Dorvin  |Finance  |  6500| 1|\n   106|Rajesh  |Finance  |  5000| 2|\n   116|Satya   |Finance  |  6500| 3|\n   118|Tejaswi |Finance  |  5500| 4|\n   102|Rajkumar|HR       |  3000| 1|\n   105|Rohit   |HR       |  3000| 2|\n   107|Preet   |HR       |  7000| 3|\n   114|Manisha |HR       |  3000| 4|\n   117|Adarsh  |HR       |  3500| 5|\n   119|Cory    |HR       |  8000| 6|\n   103|Akbar   |IT       |  4000| 1|\n   109|Sanjay  |IT       |  6500| 2|\n   110|Vasudha |IT       |  7000| 3|\n   111|Melinda |IT       |  8000| 4|\n   112|Komal   |IT       | 10000| 5|\n   115|Chandni |IT       |  4500| 6|\n   121|Rosalin |IT       |  6000| 7|\n   122|Ibrahim |IT       |  8000| 8|\n   123|Vikram  |IT       |  8000| 9|\n   124|Dheeraj |IT       | 11000|10|\n```\n\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00003-d1aefeb0-0458-4ddc-b574-30f223486c00",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "__Say we want to fetch 1st 2 employees that joined company in each dept__\n\nAssume emp_id is lower for employees who joined earlier\n\n```\nSELECT * FROM (\n\tSELECT e.*,\n\tROW_NUMBER () OVER(PARTITION BY DEPT_NAME ORDER BY emp_ID) AS rn\n\tFROM employee e \n) x\nWHERE x.rn < 3\n```\n\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n------+--------+---------+------+--+\n   101|Mohan   |Admin    |  4000| 1|\n   108|Maryam  |Admin    |  4000| 2|\n   104|Dorvin  |Finance  |  6500| 1|\n   106|Rajesh  |Finance  |  5000| 2|\n   102|Rajkumar|HR       |  3000| 1|\n   105|Rohit   |HR       |  3000| 2|\n   103|Akbar   |IT       |  4000| 1|\n   109|Sanjay  |IT       |  6500| 2|\n```\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00004-9e02339a-11f1-44d0-81e0-b6dcc40c2785",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "__Fetch top 3 employees in each dept earning max salary__\n\nWe can use the rank or dense_rank function\n\n```\nSELECT * FROM (\n\tSELECT e.*,\n\tRANK() OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rank`\n\tFROM employee e \n) x\nWHERE x.rank < 4\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rank|\n------+--------+---------+------+----+\n   120|Monica  |Admin    |  5000|   1|\n   101|Mohan   |Admin    |  4000|   2|\n   108|Maryam  |Admin    |  4000|   2|\n   104|Dorvin  |Finance  |  6500|   1|\n   116|Satya   |Finance  |  6500|   1|\n   118|Tejaswi |Finance  |  5500|   3|\n   119|Cory    |HR       |  8000|   1|\n   107|Preet   |HR       |  7000|   2|\n   117|Adarsh  |HR       |  3500|   3|\n   124|Dheeraj |IT       | 11000|   1|\n   112|Komal   |IT       | 10000|   2|\n   111|Melinda |IT       |  8000|   3|\n   122|Ibrahim |IT       |  8000|   3|\n   123|Vikram  |IT       |  8000|   3|\n```\n\n__Rank vs Dense Rank vs Row no__\n\n```\nSELECT e.*,\nRANK() OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rank`,\nDENSE_RANK () OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `dense_rank`,\nROW_NUMBER () OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rn`\nFROM employee e \n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|rank|dense_rank|rn|\n------+--------+---------+------+----+----------+--+\n   120|Monica  |Admin    |  5000|   1|         1| 1|\n   101|Mohan   |Admin    |  4000|   2|         2| 2|\n   108|Maryam  |Admin    |  4000|   2|         2| 3|\n   113|Gautham |Admin    |  2000|   4|         3| 4|\n   104|Dorvin  |Finance  |  6500|   1|         1| 1|\n   116|Satya   |Finance  |  6500|   1|         1| 2|\n   118|Tejaswi |Finance  |  5500|   3|         2| 3|\n   106|Rajesh  |Finance  |  5000|   4|         3| 4|\n   119|Cory    |HR       |  8000|   1|         1| 1|\n   107|Preet   |HR       |  7000|   2|         2| 2|\n   117|Adarsh  |HR       |  3500|   3|         3| 3|\n   102|Rajkumar|HR       |  3000|   4|         4| 4|\n   105|Rohit   |HR       |  3000|   4|         4| 5|\n   114|Manisha |HR       |  3000|   4|         4| 6|\n   124|Dheeraj |IT       | 11000|   1|         1| 1|\n   112|Komal   |IT       | 10000|   2|         2| 2|\n   111|Melinda |IT       |  8000|   3|         3| 3|\n   122|Ibrahim |IT       |  8000|   3|         3| 4|\n   123|Vikram  |IT       |  8000|   3|         3| 5|\n   110|Vasudha |IT       |  7000|   6|         4| 6|\n   109|Sanjay  |IT       |  6500|   7|         5| 7|\n   121|Rosalin |IT       |  6000|   8|         6| 8|\n   115|Chandni |IT       |  4500|   9|         7| 9|\n   103|Akbar   |IT       |  4000|  10|         8|10|\n```\n\nRank skips the next rank if there are duplicates 1->2->2->4\nDense rank does not skip ranks: 1->2->2->3\nRow number simply assigns an id to each entry, it does not care for duplicates 1->2->3->4 irrespective of duplicates\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00005-3c63e880-7211-4c8e-8014-666f6e7ab792",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "__Lead and Lag__\n\n> Also read: https://learnsql.com/blog/lead-and-lag-functions-in-sql/\n\n\nBasic Lead and Lag\n\n```\nSELECT e.*,\nLAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\nLEAD(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS next_emp_salary\nFROM employee e\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|next_emp_salary|\n------+--------+---------+------+---------------+---------------+\n   101|Mohan   |Admin    |  4000|               |           4000|\n   108|Maryam  |Admin    |  4000|           4000|           2000|\n   113|Gautham |Admin    |  2000|           4000|           5000|\n   120|Monica  |Admin    |  5000|           2000|               |\n   104|Dorvin  |Finance  |  6500|               |           5000|\n   106|Rajesh  |Finance  |  5000|           6500|           6500|\n   116|Satya   |Finance  |  6500|           5000|           5500|\n   118|Tejaswi |Finance  |  5500|           6500|               |\n   102|Rajkumar|HR       |  3000|               |           3000|\n   105|Rohit   |HR       |  3000|           3000|           7000|\n   107|Preet   |HR       |  7000|           3000|           3000|\n   114|Manisha |HR       |  3000|           7000|           3500|\n   117|Adarsh  |HR       |  3500|           3000|           8000|\n   119|Cory    |HR       |  8000|           3500|               |\n   103|Akbar   |IT       |  4000|               |           6500|\n   109|Sanjay  |IT       |  6500|           4000|           7000|\n   110|Vasudha |IT       |  7000|           6500|           8000|\n   111|Melinda |IT       |  8000|           7000|          10000|\n   112|Komal   |IT       | 10000|           8000|           4500|\n   115|Chandni |IT       |  4500|          10000|           6000|\n   121|Rosalin |IT       |  6000|           4500|           8000|\n   122|Ibrahim |IT       |  8000|           6000|           8000|\n   123|Vikram  |IT       |  8000|           8000|          11000|\n   124|Dheeraj |IT       | 11000|           8000|               |\n```\n\n\nLead and lag follow the syntax: `LAG(expression [,offset[,default_value]]) OVER(ORDER BY columns)`\n\nThese functions take three arguments: the name of the column or an expression from which the value is obtained, the number of rows to skip (offset) above, and the default value to be returned if the stored value obtained from the row above is empty. Only the first argument is required. The third argument (default value) is allowed only if you specify the second argument, the offset.\n\n```\n--- Lead and Lag\nSELECT e.*,\nLAG(SALARY, 2, -1) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\nLEAD(SALARY, 2, -1) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS next_emp_salary\nFROM employee e\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|next_emp_salary|\n------+--------+---------+------+---------------+---------------+\n   101|Mohan   |Admin    |  4000|             -1|           2000|\n   108|Maryam  |Admin    |  4000|             -1|           5000|\n   113|Gautham |Admin    |  2000|           4000|             -1|\n   120|Monica  |Admin    |  5000|           4000|             -1|\n   104|Dorvin  |Finance  |  6500|             -1|           6500|\n   106|Rajesh  |Finance  |  5000|             -1|           5500|\n   116|Satya   |Finance  |  6500|           6500|             -1|\n   118|Tejaswi |Finance  |  5500|           5000|             -1|\n   102|Rajkumar|HR       |  3000|             -1|           7000|\n   105|Rohit   |HR       |  3000|             -1|           3000|\n   107|Preet   |HR       |  7000|           3000|           3500|\n   114|Manisha |HR       |  3000|           3000|           8000|\n   117|Adarsh  |HR       |  3500|           7000|             -1|\n   119|Cory    |HR       |  8000|           3000|             -1|\n   103|Akbar   |IT       |  4000|             -1|           7000|\n   109|Sanjay  |IT       |  6500|             -1|           8000|\n   110|Vasudha |IT       |  7000|           4000|          10000|\n   111|Melinda |IT       |  8000|           6500|           4500|\n   112|Komal   |IT       | 10000|           7000|           6000|\n   115|Chandni |IT       |  4500|           8000|           8000|\n   121|Rosalin |IT       |  6000|          10000|           8000|\n   122|Ibrahim |IT       |  8000|           4500|          11000|\n   123|Vikram  |IT       |  8000|           6000|             -1|\n   124|Dheeraj |IT       | 11000|           8000|             -1|\n```\n\ncompare salary of each employee with prev one in the dept:\n\n```\nSELECT e.*,\nLAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\nCASE WHEN e.SALARY > LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'higher than prev'\n\tWHEN e.SALARY < LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'lower than prev'\n\tWHEN e.SALARY = LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'same as prev'\nEND salary_comparison\nFROM employee e\n```\n\n```\nemp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|salary_comparison|\n------+--------+---------+------+---------------+-----------------+\n   101|Mohan   |Admin    |  4000|               |                 |\n   108|Maryam  |Admin    |  4000|           4000|     same as prev|\n   113|Gautham |Admin    |  2000|           4000|  lower than prev|\n   120|Monica  |Admin    |  5000|           2000| higher than prev|\n   104|Dorvin  |Finance  |  6500|               |                 |\n   106|Rajesh  |Finance  |  5000|           6500|  lower than prev|\n   116|Satya   |Finance  |  6500|           5000| higher than prev|\n   118|Tejaswi |Finance  |  5500|           6500|  lower than prev|\n   102|Rajkumar|HR       |  3000|               |                 |\n   105|Rohit   |HR       |  3000|           3000|     same as prev|\n   107|Preet   |HR       |  7000|           3000| higher than prev|\n   114|Manisha |HR       |  3000|           7000|  lower than prev|\n   117|Adarsh  |HR       |  3500|           3000| higher than prev|\n   119|Cory    |HR       |  8000|           3500| higher than prev|\n   103|Akbar   |IT       |  4000|               |                 |\n   109|Sanjay  |IT       |  6500|           4000| higher than prev|\n   110|Vasudha |IT       |  7000|           6500| higher than prev|\n   111|Melinda |IT       |  8000|           7000| higher than prev|\n   112|Komal   |IT       | 10000|           8000| higher than prev|\n   115|Chandni |IT       |  4500|          10000|  lower than prev|\n   121|Rosalin |IT       |  6000|           4500| higher than prev|\n   122|Ibrahim |IT       |  8000|           6000| higher than prev|\n   123|Vikram  |IT       |  8000|           8000|     same as prev|\n   124|Dheeraj |IT       | 11000|           8000| higher than prev|\n```\n",
   "metadata": {
    "tags": [],
    "cell_id": "00006-84494bf8-197c-41cf-b08a-e4906b568062",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### SQL Question 1\n\n> https://platform.stratascratch.com/coding/9899-percentage-of-total-spend?python&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n\n```\n--- INNER JOIN AS WE WANT ONLY CUSTOMERS WHO HAVE PLACED AN ORDER\n\nselect first_name,order_details,\ntotal_order_cost/SUM(total_order_cost) OVER(PARTITION BY first_name) AS \"percentage of the order cost\"\n\nfrom orders o\nINNER JOIN customers c\nON o.cust_id = c.id\n```\n\n#### SQL Question 2\n\n> https://platform.stratascratch.com/coding/2036-lowest-revenue-generated-restaurants?python&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n\n```\n--- Filter data to only use May 2020 records\nSELECT * FROM (\nSELECT \n    restaurant_id,\n    order_total,\n    NTILE(100) OVER (ORDER BY order_total ASC) AS percentile_value\nFROM (\n\nSELECT \n    --EXTRACT(MONTH FROM customer_placed_order_datetime) as order_month,\n    --EXTRACT(YEAR FROM customer_placed_order_datetime) as order_year,\n    restaurant_id, \n    SUM(order_total) AS order_total\nFROM doordash_delivery\nWHERE EXTRACT(MONTH FROM customer_placed_order_datetime) = 5 and EXTRACT(YEAR FROM customer_placed_order_datetime) = 2020\nGROUP BY restaurant_id\n) as sub1\n) as sub0\nWHERE percentile_value < 3\nORDER BY 2 ASC\n```\n\nInstead of a subquery we can use a CTE, which is a bit easier to interpret\nAlse CTE creates temp tables, so we can re-use the query later\n",
   "metadata": {
    "tags": [],
    "cell_id": "00007-1cbcb41f-c94e-4d5d-97b7-5f78cfddcaaf",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Leetcode 180. Consecutive Numbers\n\n> https://leetcode.com/problems/consecutive-numbers/\n\n```sql\nWITH CTE AS (\nSELECT\nnum,\nLAG(num, 1) OVER() AS `prev_1`,\nLAG(num, 2) OVER() AS `prev_2`\nFROM \nLogs\n),\nCTE2 AS (\n    SELECT\n    CASE WHEN num = prev_1 and num = prev_2 THEN num\n    ELSE NULL END AS `ConsecutiveNums`\n    FROM CTE\n)\n\nSELECT DISTINCT ConsecutiveNums FROM CTE2\nWHERE ConsecutiveNums IS NOT NULL\n```",
   "metadata": {
    "tags": [],
    "cell_id": "00008-33bfcba5-46be-40a8-bb9b-b069c2c878fa",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Leetcode 262. Trips and Users\n\n> https://leetcode.com/problems/trips-and-users/\n\n```sql\n\n# Write your MySQL query statement below\nWITH CTE AS (\nSELECT request_at, status, u.banned AS user_banned, u1.banned AS driver_banned\nFROM Trips t\nINNER JOIN Users u\nON t.client_id = u.users_id\nINNER JOIN Users u1 \nON t.driver_id = u1.users_id\n    \nWHERE u.banned = \"No\" and u1.banned = \"No\" and request_at BETWEEN \"2013-10-01\" and \"2013-10-03\"\n),\n\nCTE2 AS\n(\nSELECT request_at,\nCASE WHEN status LIKE \"cancelled%\" THEN 1\nELSE 0 END AS new_status\nFROM CTE\n),\nCTE3 AS \n(\nSELECT \n    request_at AS Day,\n    ROUND(SUM(new_status)/COUNT(new_status), 2) AS `Cancellation Rate`\n    FROM CTE2\n    GROUP BY request_at\n)\n\nSELECT * FROM CTE3\n\n```",
   "metadata": {
    "tags": [],
    "cell_id": "00009-1686b787-49d0-4892-a03e-1be85c32029d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## SQL Moving Averages\n\n> https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/\n---\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00008-3a7f2aa0-b931-481b-93f6-0d3b646e8487",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00009-406f8cc9-d986-4c27-8dda-a869d51f58e0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4546c55d-0fa0-4550-8db3-cc7c2a668064' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "3f480396-3090-4c9d-95d9-b94e0489f5a1",
  "deepnote_execution_queue": []
 }
}