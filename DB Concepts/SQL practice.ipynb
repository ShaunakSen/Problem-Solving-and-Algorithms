{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-ee9233e5-082b-4a19-8097-b2e66ba0e976",
    "deepnote_cell_height": 1491.25,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## SQL Practice\n",
    "\n",
    "> Based on notes on: https://www.youtube.com/watch?v=gwp3dJUsy5g\n",
    "\n",
    "---\n",
    "\n",
    "### SQL Order of execution\n",
    "\n",
    "> https://sqlbolt.com/lesson/select_queries_order_of_execution\n",
    "\n",
    "```sql\n",
    "SELECT DISTINCT column, AGG_FUNC(column_or_expression), …\n",
    "FROM mytable\n",
    "    JOIN another_table\n",
    "      ON mytable.column = another_table.column\n",
    "    WHERE constraint_expression\n",
    "    GROUP BY column\n",
    "    HAVING constraint_expression\n",
    "    ORDER BY column ASC/DESC\n",
    "    LIMIT count OFFSET COUNT;\n",
    "```\n",
    "\n",
    "\n",
    "1. FROM and JOIN\n",
    "\n",
    "The FROM clause, and subsequent JOINs are first executed to determine the total working set of data that is being queried. This includes subqueries in this clause, and can cause temporary tables to be created under the hood containing all the columns and rows of the tables being joined.\n",
    "\n",
    "2. WHERE\n",
    "\n",
    "Once we have the total working set of data, the first-pass WHERE constraints are applied to the individual rows, and rows that do not satisfy the constraint are discarded. Each of the constraints can only access columns directly from the tables requested in the FROM clause. __Aliases in the SELECT part of the query are not accessible in most databases since they may include expressions dependent on parts of the query that have not yet executed.__\n",
    "\n",
    "3. GROUP BY\n",
    "\n",
    "The remaining rows after the WHERE constraints are applied are then grouped based on common values in the column specified in the GROUP BY clause. __As a result of the grouping, there will only be as many rows as there are unique values in that column__. Implicitly, this means that you should only need to use this when you have aggregate functions in your query.\n",
    "\n",
    "4. HAVING\n",
    "\n",
    "If the query has a GROUP BY clause, then the constraints in the HAVING clause are then applied to the grouped rows, discard the grouped rows that don't satisfy the constraint. __Like the WHERE clause, aliases are also not accessible from this step in most databases__.\n",
    "\n",
    "5. SELECT\n",
    "\n",
    "Any expressions in the SELECT part of the query are finally computed.\n",
    "\n",
    "6. DISTINCT\n",
    "\n",
    "Of the remaining rows, rows with duplicate values in the column marked as DISTINCT will be discarded.\n",
    "\n",
    "7. ORDER BY\n",
    "\n",
    "If an order is specified by the ORDER BY clause, the rows are then sorted by the specified data in either ascending or descending order. Since all the expressions in the SELECT part of the query have been computed, you can reference aliases in this clause.\n",
    "\n",
    "\n",
    "8. LIMIT / OFFSET\n",
    "\n",
    "Finally, the rows that fall outside the range specified by the LIMIT and OFFSET are discarded, leaving the final set of rows to be returned from the query.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Difference bw DISTINCT and GROUP BY\n",
    "\n",
    "> https://www.sitepoint.com/community/t/mysql-when-group-by-is-faster-than-distinct/272420\n",
    "\n",
    "In some cases GROUP BY might be faster than DISTICT\n",
    "\n",
    "- When you run DISTINCT MySQL has to looks across all selected columns whereas GROUP BY will only do it for whatever columns you explicitly assign to GROUP BY so there is less work to do (my query was selecting about 15 columns)\n",
    "\n",
    "this is true as far as it goes… however, if you have GROUP BY with only a few of your 15 columns, then technically you are running an invalid query in mysql (yes, mysql will run these invalid queries), and the results are indeterminate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-5c35158c-6a2a-4e0e-95ac-2a9be709c9c7",
    "deepnote_cell_height": 1373.640625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Combining Data\n",
    "\n",
    "#### Unions\n",
    "\n",
    "Put simply, a union (SQL Union) is the process of stacking two tables on top of one another. You will usually do this when your data is split up into multiple sections like an excel spreadsheet of a year’s sales split by month.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa5adad37-92c6-4924-b52c-2ab340ae351b%2FScreen_Shot_2021-03-15_at_12.23.40_PM.png?table=block&id=199c2a41-408a-4898-a329-c7810d5d9a39&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1390&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "#### Joins\n",
    "\n",
    "Joins combine two tables horizontally. For a join, like a Union you have to have at least two tables, what we call our Left Table and our Right Table. You (mostly) have to have at least one matching column between the two tables, and you will match rows from these columns. The most common way to visualize the types of Joins are through Venn Diagrams.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fba7b4d86-db23-4142-b860-2e23fce3bae4%2FScreen_Shot_2021-03-15_at_12.36.03_PM.png?table=block&id=31a987e2-12d3-4d95-bdbc-a37449be2b29&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=2000&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe94f8697-5537-4458-9992-4ea02defbea0%2FScreen_Shot_2021-03-15_at_12.36.37_PM.png?table=block&id=91c32502-734f-4ef3-90b6-d9725da9cd3b&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=960&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "__Inner Join__\n",
    "\n",
    "We’re now going to do something called an Inner Join on the [ID] column which will only output exact matches from the [ID] column in our output.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1d7856a8-7fc5-4673-a7a6-522d5fd52a9c%2FScreen_Shot_2021-03-15_at_12.37.59_PM.png?table=block&id=b5c56e80-bb05-4dc5-955b-073c3d231bfc&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1370&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "__Left Join__\n",
    "\n",
    "A Left Join keeps all of the data from your Left table and whatever matches from the Right table.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa4b067dc-dd79-4456-8cae-2f81a8ac4201%2FScreen_Shot_2021-03-15_at_12.39.07_PM.png?table=block&id=f0f911be-573d-415e-b829-873cd6c300c1&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1350&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "__Right Join__\n",
    "\n",
    "A Right Join does the exact opposite and keeps everything from your Right table while only bringing in the matches from the Left table.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1235ef85-ef65-4e03-ba26-6661b39bad62%2FScreen_Shot_2021-03-15_at_12.39.43_PM.png?table=block&id=fd1880dd-7d29-477b-b654-7e22a9b9873c&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "__Full Join__\n",
    "\n",
    "A Full Join brings in everything from both tables and matches whatever will match from the columns you specify.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe42b3cca-c8df-48fb-9b63-211c498be6a9%2FScreen_Shot_2021-03-15_at_12.40.52_PM.png?table=block&id=02ceefe0-20a9-48f7-831d-88698f314d43&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "__Cross Join__\n",
    "\n",
    "The CROSS JOIN is used to generate a paired combination of each row of the first table with each row of the second table. This join type is also known as cartesian join.\n",
    "\n",
    "Suppose that we are sitting in a coffee shop and we decide to order breakfast. Shortly, we will look at the menu and we will start thinking of which meal and drink combination could be more tastier. Our brain will receive this signal and begin to generate all meal and drink combinations.\n",
    "\n",
    "The following image illustrates all menu combinations that can be generated by our brain. The SQL CROSS JOIN works similarly to this mechanism, as it creates all paired combinations of the rows of the tables that will be joined.\n",
    "\n",
    "![](https://www.sqlshack.com/wp-content/uploads/2020/02/sql-cross-join-working-mechanism.png)\n",
    "\n",
    "The SQL queries which contain the CROSS JOIN keyword can be very costly as it will need nested loops. We try to say that these queries have a high potential to consume more resources and can cause performance issues.\n",
    "\n",
    "Briefly, when we decide to use the CROSS JOIN in any query, we should consider the number of the tables that will be joined. Such as, when we CROSS JOIN two tables and if the first one contains 1000 rows and the second one contains 1000 rows, the row count of the resultset will be 1.000.000 rows.\n",
    "\n",
    "---\n",
    "\n",
    "Joins can get a bit tricky because of the potential for gotchas when joining two tables. The most common one is row duplication where you accidentally duplicate rows because the columns you’re matching on have multiple potential matches. In the example below we’re going to try an Inner Join. You’ll notice the columns in Orange were duplicated.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F89439f1d-e24a-4451-bd0d-37a0dfcc4487%2FScreen_Shot_2021-03-15_at_12.42.17_PM.png?table=block&id=9bac2f13-52ee-4ac9-9bc8-806a6d7187d0&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1370&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n",
    "This isn’t an error per se but it is something to watch out for as it can cause you to duplicate data you don’t intend to duplicate.\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F8d64f5ba-d067-40f5-94bb-32beb9fe49d1%2FScreen_Shot_2021-03-15_at_12.42.37_PM.png?table=block&id=6a39c3e9-2900-4be6-9dc2-bbb14f68181a&spaceId=691f8197-dec0-4338-b1a8-a47162b151ba&width=1360&userId=bdc14b6b-7340-420b-85e2-540dbef29bc8&cache=v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-ddb51947-8335-4cf1-9437-27c50713072e",
    "deepnote_cell_height": 361.5,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### LIKE, BETWEEN, IN\n",
    "\n",
    "https://www.w3schools.com/sql/sql_wildcards.asp\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "FROM dataset_1\n",
    "WHERE weather LIKE 'Sun%';\n",
    "```\n",
    "\n",
    "```\n",
    "SELECT DISTINCT temperature \n",
    "FROM dataset_1\n",
    "WHERE temperature BETWEEN 29 AND 75;\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "SELECT occupation\n",
    "FROM dataset_1\n",
    "WHERE occupation IN ('Sales & Related', 'Management');\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-d1aefeb0-0458-4ddc-b574-30f223486c00",
    "deepnote_cell_height": 3063.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Window functions\n",
    "\n",
    "> Based on tutorial : https://www.youtube.com/watch?v=Ww71knvhQ-s\n",
    "\n",
    "---\n",
    "\n",
    "We create an `employee` table\n",
    "\n",
    "![](https://i.imgur.com/dehIA2J.png)\n",
    "\n",
    "\n",
    "what i want is all the cols from employee table along with a max_salary column, which displays the overall max salary\n",
    "\n",
    "```\n",
    "SELECT e.*,\n",
    "MAX(SALARY) OVER() as max_salary \n",
    "FROM employee e;\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|max_salary|\n",
    "------+--------+---------+------+----------+\n",
    "   101|Mohan   |Admin    |  4000|     11000|\n",
    "   102|Rajkumar|HR       |  3000|     11000|\n",
    "   103|Akbar   |IT       |  4000|     11000|\n",
    "   104|Dorvin  |Finance  |  6500|     11000|\n",
    "   105|Rohit   |HR       |  3000|     11000|\n",
    "   106|Rajesh  |Finance  |  5000|     11000|\n",
    "   107|Preet   |HR       |  7000|     11000|\n",
    "   108|Maryam  |Admin    |  4000|     11000|\n",
    "   109|Sanjay  |IT       |  6500|     11000|\n",
    "   110|Vasudha |IT       |  7000|     11000|\n",
    "   111|Melinda |IT       |  8000|     11000|\n",
    "   112|Komal   |IT       | 10000|     11000|\n",
    "   113|Gautham |Admin    |  2000|     11000|\n",
    "   114|Manisha |HR       |  3000|     11000|\n",
    "   115|Chandni |IT       |  4500|     11000|\n",
    "   116|Satya   |Finance  |  6500|     11000|\n",
    "   117|Adarsh  |HR       |  3500|     11000|\n",
    "   118|Tejaswi |Finance  |  5500|     11000|\n",
    "   119|Cory    |HR       |  8000|     11000|\n",
    "   120|Monica  |Admin    |  5000|     11000|\n",
    "   121|Rosalin |IT       |  6000|     11000|\n",
    "   122|Ibrahim |IT       |  8000|     11000|\n",
    "   123|Vikram  |IT       |  8000|     11000|\n",
    "   124|Dheeraj |IT       | 11000|     11000|\n",
    "   ````\n",
    "\n",
    "Since we are using an `OVER` clause, SQL does not tream max as an agg function, it will treat it as a window function. But we have not specified any col in the over clause, so it will consider a window over the entire dataset\n",
    "\n",
    "\n",
    "Now we want the max salary for each dept along with other cols\n",
    "\n",
    "```\n",
    "SELECT e.*,\n",
    "MAX(SALARY) OVER(PARTION BY DEPT_NAME) as max_salary \n",
    "FROM employee e;\n",
    "```\n",
    "\n",
    "Now for every distinct value of dept, sql creates a window and calculates the max salary for that window\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|max_salary|\n",
    "------+--------+---------+------+----------+\n",
    "   101|Mohan   |Admin    |  4000|      5000|\n",
    "   108|Maryam  |Admin    |  4000|      5000|\n",
    "   113|Gautham |Admin    |  2000|      5000|\n",
    "   120|Monica  |Admin    |  5000|      5000|\n",
    "   104|Dorvin  |Finance  |  6500|      6500|\n",
    "   106|Rajesh  |Finance  |  5000|      6500|\n",
    "   116|Satya   |Finance  |  6500|      6500|\n",
    "   118|Tejaswi |Finance  |  5500|      6500|\n",
    "   102|Rajkumar|HR       |  3000|      8000|\n",
    "   105|Rohit   |HR       |  3000|      8000|\n",
    "   107|Preet   |HR       |  7000|      8000|\n",
    "   114|Manisha |HR       |  3000|      8000|\n",
    "   117|Adarsh  |HR       |  3500|      8000|\n",
    "   119|Cory    |HR       |  8000|      8000|\n",
    "   103|Akbar   |IT       |  4000|     11000|\n",
    "   109|Sanjay  |IT       |  6500|     11000|\n",
    "   110|Vasudha |IT       |  7000|     11000|\n",
    "   111|Melinda |IT       |  8000|     11000|\n",
    "   112|Komal   |IT       | 10000|     11000|\n",
    "   115|Chandni |IT       |  4500|     11000|\n",
    "   121|Rosalin |IT       |  6000|     11000|\n",
    "   122|Ibrahim |IT       |  8000|     11000|\n",
    "   123|Vikram  |IT       |  8000|     11000|\n",
    "   124|Dheeraj |IT       | 11000|     11000|\n",
    "```\n",
    "\n",
    "We can use MAX, MIN, COUNT, SUM - the agg functions we use with GROUP BY\n",
    "\n",
    "But there are some specific window functions as well:\n",
    "\n",
    "#### Row number\n",
    "\n",
    "This simply assigns an id to every record in our table\n",
    "\n",
    "```\n",
    "SELECT e.*,\n",
    "ROW_NUMBER () OVER() AS rn\n",
    "FROM employee e \n",
    "```\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n",
    "------+--------+---------+------+--+\n",
    "   101|Mohan   |Admin    |  4000| 1|\n",
    "   102|Rajkumar|HR       |  3000| 2|\n",
    "   103|Akbar   |IT       |  4000| 3|\n",
    "   104|Dorvin  |Finance  |  6500| 4|\n",
    "   105|Rohit   |HR       |  3000| 5|\n",
    "   106|Rajesh  |Finance  |  5000| 6|\n",
    "   107|Preet   |HR       |  7000| 7|\n",
    "   108|Maryam  |Admin    |  4000| 8|\n",
    "   109|Sanjay  |IT       |  6500| 9|\n",
    "   110|Vasudha |IT       |  7000|10|\n",
    "   111|Melinda |IT       |  8000|11|\n",
    "   112|Komal   |IT       | 10000|12|\n",
    "   113|Gautham |Admin    |  2000|13|\n",
    "   114|Manisha |HR       |  3000|14|\n",
    "   115|Chandni |IT       |  4500|15|\n",
    "   116|Satya   |Finance  |  6500|16|\n",
    "   117|Adarsh  |HR       |  3500|17|\n",
    "   118|Tejaswi |Finance  |  5500|18|\n",
    "   119|Cory    |HR       |  8000|19|\n",
    "   120|Monica  |Admin    |  5000|20|\n",
    "   121|Rosalin |IT       |  6000|21|\n",
    "   122|Ibrahim |IT       |  8000|22|\n",
    "   123|Vikram  |IT       |  8000|23|\n",
    "   124|Dheeraj |IT       | 11000|24|\n",
    "```\n",
    "\n",
    "```\n",
    "SELECT e.*,\n",
    "ROW_NUMBER () OVER(PARTITION BY DEPT_NAME) AS rn\n",
    "FROM employee e \n",
    "```\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n",
    "------+--------+---------+------+--+\n",
    "   101|Mohan   |Admin    |  4000| 1|\n",
    "   108|Maryam  |Admin    |  4000| 2|\n",
    "   113|Gautham |Admin    |  2000| 3|\n",
    "   120|Monica  |Admin    |  5000| 4|\n",
    "   104|Dorvin  |Finance  |  6500| 1|\n",
    "   106|Rajesh  |Finance  |  5000| 2|\n",
    "   116|Satya   |Finance  |  6500| 3|\n",
    "   118|Tejaswi |Finance  |  5500| 4|\n",
    "   102|Rajkumar|HR       |  3000| 1|\n",
    "   105|Rohit   |HR       |  3000| 2|\n",
    "   107|Preet   |HR       |  7000| 3|\n",
    "   114|Manisha |HR       |  3000| 4|\n",
    "   117|Adarsh  |HR       |  3500| 5|\n",
    "   119|Cory    |HR       |  8000| 6|\n",
    "   103|Akbar   |IT       |  4000| 1|\n",
    "   109|Sanjay  |IT       |  6500| 2|\n",
    "   110|Vasudha |IT       |  7000| 3|\n",
    "   111|Melinda |IT       |  8000| 4|\n",
    "   112|Komal   |IT       | 10000| 5|\n",
    "   115|Chandni |IT       |  4500| 6|\n",
    "   121|Rosalin |IT       |  6000| 7|\n",
    "   122|Ibrahim |IT       |  8000| 8|\n",
    "   123|Vikram  |IT       |  8000| 9|\n",
    "   124|Dheeraj |IT       | 11000|10|\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-9e02339a-11f1-44d0-81e0-b6dcc40c2785",
    "deepnote_cell_height": 475.46875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "__Say we want to fetch 1st 2 employees that joined company in each dept__\n",
    "\n",
    "Assume emp_id is lower for employees who joined earlier\n",
    "\n",
    "```\n",
    "SELECT * FROM (\n",
    "\tSELECT e.*,\n",
    "\tROW_NUMBER () OVER(PARTITION BY DEPT_NAME ORDER BY emp_ID) AS rn\n",
    "\tFROM employee e \n",
    ") x\n",
    "WHERE x.rn < 3\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|rn|\n",
    "------+--------+---------+------+--+\n",
    "   101|Mohan   |Admin    |  4000| 1|\n",
    "   108|Maryam  |Admin    |  4000| 2|\n",
    "   104|Dorvin  |Finance  |  6500| 1|\n",
    "   106|Rajesh  |Finance  |  5000| 2|\n",
    "   102|Rajkumar|HR       |  3000| 1|\n",
    "   105|Rohit   |HR       |  3000| 2|\n",
    "   103|Akbar   |IT       |  4000| 1|\n",
    "   109|Sanjay  |IT       |  6500| 2|\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-3c63e880-7211-4c8e-8014-666f6e7ab792",
    "deepnote_cell_height": 1366.09375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "__Fetch top 3 employees in each dept earning max salary__\n",
    "\n",
    "We can use the rank or dense_rank function\n",
    "\n",
    "```\n",
    "SELECT * FROM (\n",
    "\tSELECT e.*,\n",
    "\tRANK() OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rank`\n",
    "\tFROM employee e \n",
    ") x\n",
    "WHERE x.rank < 4\n",
    "```\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|rank|\n",
    "------+--------+---------+------+----+\n",
    "   120|Monica  |Admin    |  5000|   1|\n",
    "   101|Mohan   |Admin    |  4000|   2|\n",
    "   108|Maryam  |Admin    |  4000|   2|\n",
    "   104|Dorvin  |Finance  |  6500|   1|\n",
    "   116|Satya   |Finance  |  6500|   1|\n",
    "   118|Tejaswi |Finance  |  5500|   3|\n",
    "   119|Cory    |HR       |  8000|   1|\n",
    "   107|Preet   |HR       |  7000|   2|\n",
    "   117|Adarsh  |HR       |  3500|   3|\n",
    "   124|Dheeraj |IT       | 11000|   1|\n",
    "   112|Komal   |IT       | 10000|   2|\n",
    "   111|Melinda |IT       |  8000|   3|\n",
    "   122|Ibrahim |IT       |  8000|   3|\n",
    "   123|Vikram  |IT       |  8000|   3|\n",
    "```\n",
    "\n",
    "__Rank vs Dense Rank vs Row no__\n",
    "\n",
    "```\n",
    "SELECT e.*,\n",
    "RANK() OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rank`,\n",
    "DENSE_RANK () OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `dense_rank`,\n",
    "ROW_NUMBER () OVER(PARTITION BY DEPT_NAME ORDER BY SALARY DESC) AS `rn`\n",
    "FROM employee e \n",
    "```\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|rank|dense_rank|rn|\n",
    "------+--------+---------+------+----+----------+--+\n",
    "   120|Monica  |Admin    |  5000|   1|         1| 1|\n",
    "   101|Mohan   |Admin    |  4000|   2|         2| 2|\n",
    "   108|Maryam  |Admin    |  4000|   2|         2| 3|\n",
    "   113|Gautham |Admin    |  2000|   4|         3| 4|\n",
    "   104|Dorvin  |Finance  |  6500|   1|         1| 1|\n",
    "   116|Satya   |Finance  |  6500|   1|         1| 2|\n",
    "   118|Tejaswi |Finance  |  5500|   3|         2| 3|\n",
    "   106|Rajesh  |Finance  |  5000|   4|         3| 4|\n",
    "   119|Cory    |HR       |  8000|   1|         1| 1|\n",
    "   107|Preet   |HR       |  7000|   2|         2| 2|\n",
    "   117|Adarsh  |HR       |  3500|   3|         3| 3|\n",
    "   102|Rajkumar|HR       |  3000|   4|         4| 4|\n",
    "   105|Rohit   |HR       |  3000|   4|         4| 5|\n",
    "   114|Manisha |HR       |  3000|   4|         4| 6|\n",
    "   124|Dheeraj |IT       | 11000|   1|         1| 1|\n",
    "   112|Komal   |IT       | 10000|   2|         2| 2|\n",
    "   111|Melinda |IT       |  8000|   3|         3| 3|\n",
    "   122|Ibrahim |IT       |  8000|   3|         3| 4|\n",
    "   123|Vikram  |IT       |  8000|   3|         3| 5|\n",
    "   110|Vasudha |IT       |  7000|   6|         4| 6|\n",
    "   109|Sanjay  |IT       |  6500|   7|         5| 7|\n",
    "   121|Rosalin |IT       |  6000|   8|         6| 8|\n",
    "   115|Chandni |IT       |  4500|   9|         7| 9|\n",
    "   103|Akbar   |IT       |  4000|  10|         8|10|\n",
    "```\n",
    "\n",
    "Rank skips the next rank if there are duplicates 1->2->2->4\n",
    "Dense rank does not skip ranks: 1->2->2->3\n",
    "Row number simply assigns an id to each entry, it does not care for duplicates 1->2->3->4 irrespective of duplicates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-84494bf8-197c-41cf-b08a-e4906b568062",
    "deepnote_cell_height": 2367.890625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "__Lead and Lag__\n",
    "\n",
    "> Also read: https://learnsql.com/blog/lead-and-lag-functions-in-sql/\n",
    "\n",
    "\n",
    "Basic Lead and Lag\n",
    "\n",
    "```\n",
    "SELECT e.*,\n",
    "LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\n",
    "LEAD(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS next_emp_salary\n",
    "FROM employee e\n",
    "```\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|next_emp_salary|\n",
    "------+--------+---------+------+---------------+---------------+\n",
    "   101|Mohan   |Admin    |  4000|               |           4000|\n",
    "   108|Maryam  |Admin    |  4000|           4000|           2000|\n",
    "   113|Gautham |Admin    |  2000|           4000|           5000|\n",
    "   120|Monica  |Admin    |  5000|           2000|               |\n",
    "   104|Dorvin  |Finance  |  6500|               |           5000|\n",
    "   106|Rajesh  |Finance  |  5000|           6500|           6500|\n",
    "   116|Satya   |Finance  |  6500|           5000|           5500|\n",
    "   118|Tejaswi |Finance  |  5500|           6500|               |\n",
    "   102|Rajkumar|HR       |  3000|               |           3000|\n",
    "   105|Rohit   |HR       |  3000|           3000|           7000|\n",
    "   107|Preet   |HR       |  7000|           3000|           3000|\n",
    "   114|Manisha |HR       |  3000|           7000|           3500|\n",
    "   117|Adarsh  |HR       |  3500|           3000|           8000|\n",
    "   119|Cory    |HR       |  8000|           3500|               |\n",
    "   103|Akbar   |IT       |  4000|               |           6500|\n",
    "   109|Sanjay  |IT       |  6500|           4000|           7000|\n",
    "   110|Vasudha |IT       |  7000|           6500|           8000|\n",
    "   111|Melinda |IT       |  8000|           7000|          10000|\n",
    "   112|Komal   |IT       | 10000|           8000|           4500|\n",
    "   115|Chandni |IT       |  4500|          10000|           6000|\n",
    "   121|Rosalin |IT       |  6000|           4500|           8000|\n",
    "   122|Ibrahim |IT       |  8000|           6000|           8000|\n",
    "   123|Vikram  |IT       |  8000|           8000|          11000|\n",
    "   124|Dheeraj |IT       | 11000|           8000|               |\n",
    "```\n",
    "\n",
    "\n",
    "Lead and lag follow the syntax: `LAG(expression [,offset[,default_value]]) OVER(ORDER BY columns)`\n",
    "\n",
    "These functions take three arguments: the name of the column or an expression from which the value is obtained, the number of rows to skip (offset) above, and the default value to be returned if the stored value obtained from the row above is empty. Only the first argument is required. The third argument (default value) is allowed only if you specify the second argument, the offset.\n",
    "\n",
    "```\n",
    "--- Lead and Lag\n",
    "SELECT e.*,\n",
    "LAG(SALARY, 2, -1) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\n",
    "LEAD(SALARY, 2, -1) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS next_emp_salary\n",
    "FROM employee e\n",
    "```\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|next_emp_salary|\n",
    "------+--------+---------+------+---------------+---------------+\n",
    "   101|Mohan   |Admin    |  4000|             -1|           2000|\n",
    "   108|Maryam  |Admin    |  4000|             -1|           5000|\n",
    "   113|Gautham |Admin    |  2000|           4000|             -1|\n",
    "   120|Monica  |Admin    |  5000|           4000|             -1|\n",
    "   104|Dorvin  |Finance  |  6500|             -1|           6500|\n",
    "   106|Rajesh  |Finance  |  5000|             -1|           5500|\n",
    "   116|Satya   |Finance  |  6500|           6500|             -1|\n",
    "   118|Tejaswi |Finance  |  5500|           5000|             -1|\n",
    "   102|Rajkumar|HR       |  3000|             -1|           7000|\n",
    "   105|Rohit   |HR       |  3000|             -1|           3000|\n",
    "   107|Preet   |HR       |  7000|           3000|           3500|\n",
    "   114|Manisha |HR       |  3000|           3000|           8000|\n",
    "   117|Adarsh  |HR       |  3500|           7000|             -1|\n",
    "   119|Cory    |HR       |  8000|           3000|             -1|\n",
    "   103|Akbar   |IT       |  4000|             -1|           7000|\n",
    "   109|Sanjay  |IT       |  6500|             -1|           8000|\n",
    "   110|Vasudha |IT       |  7000|           4000|          10000|\n",
    "   111|Melinda |IT       |  8000|           6500|           4500|\n",
    "   112|Komal   |IT       | 10000|           7000|           6000|\n",
    "   115|Chandni |IT       |  4500|           8000|           8000|\n",
    "   121|Rosalin |IT       |  6000|          10000|           8000|\n",
    "   122|Ibrahim |IT       |  8000|           4500|          11000|\n",
    "   123|Vikram  |IT       |  8000|           6000|             -1|\n",
    "   124|Dheeraj |IT       | 11000|           8000|             -1|\n",
    "```\n",
    "\n",
    "compare salary of each employee with prev one in the dept:\n",
    "\n",
    "```\n",
    "SELECT e.*,\n",
    "LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) AS prev_emp_salary,\n",
    "CASE WHEN e.SALARY > LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'higher than prev'\n",
    "\tWHEN e.SALARY < LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'lower than prev'\n",
    "\tWHEN e.SALARY = LAG(SALARY) OVER (PARTITION BY DEPT_NAME ORDER BY emp_ID) THEN 'same as prev'\n",
    "END salary_comparison\n",
    "FROM employee e\n",
    "```\n",
    "\n",
    "```\n",
    "emp_ID|emp_NAME|DEPT_NAME|SALARY|prev_emp_salary|salary_comparison|\n",
    "------+--------+---------+------+---------------+-----------------+\n",
    "   101|Mohan   |Admin    |  4000|               |                 |\n",
    "   108|Maryam  |Admin    |  4000|           4000|     same as prev|\n",
    "   113|Gautham |Admin    |  2000|           4000|  lower than prev|\n",
    "   120|Monica  |Admin    |  5000|           2000| higher than prev|\n",
    "   104|Dorvin  |Finance  |  6500|               |                 |\n",
    "   106|Rajesh  |Finance  |  5000|           6500|  lower than prev|\n",
    "   116|Satya   |Finance  |  6500|           5000| higher than prev|\n",
    "   118|Tejaswi |Finance  |  5500|           6500|  lower than prev|\n",
    "   102|Rajkumar|HR       |  3000|               |                 |\n",
    "   105|Rohit   |HR       |  3000|           3000|     same as prev|\n",
    "   107|Preet   |HR       |  7000|           3000| higher than prev|\n",
    "   114|Manisha |HR       |  3000|           7000|  lower than prev|\n",
    "   117|Adarsh  |HR       |  3500|           3000| higher than prev|\n",
    "   119|Cory    |HR       |  8000|           3500| higher than prev|\n",
    "   103|Akbar   |IT       |  4000|               |                 |\n",
    "   109|Sanjay  |IT       |  6500|           4000| higher than prev|\n",
    "   110|Vasudha |IT       |  7000|           6500| higher than prev|\n",
    "   111|Melinda |IT       |  8000|           7000| higher than prev|\n",
    "   112|Komal   |IT       | 10000|           8000| higher than prev|\n",
    "   115|Chandni |IT       |  4500|          10000|  lower than prev|\n",
    "   121|Rosalin |IT       |  6000|           4500| higher than prev|\n",
    "   122|Ibrahim |IT       |  8000|           6000| higher than prev|\n",
    "   123|Vikram  |IT       |  8000|           8000|     same as prev|\n",
    "   124|Dheeraj |IT       | 11000|           8000| higher than prev|\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c332944a-dffa-4476-aa71-1f5aeb738e1b",
    "deepnote_cell_height": 1502.078125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Finding streaks\n",
    "\n",
    "> https://www.youtube.com/watch?v=VEjxlKBkZGM\n",
    "---\n",
    "\n",
    "Imagine a player had the following results:\n",
    "\n",
    "![](https://i.imgur.com/ZfBBlqM.png)\n",
    "\n",
    "The question is how do we detect these 2 streaks\n",
    "\n",
    "The method we use here is\n",
    "\n",
    "- Add row no for wins and losses\n",
    "- Add row no for wins separately and loss separately\n",
    "- subtract the 2\n",
    "\n",
    "![](https://i.imgur.com/HhXfnny.png)\n",
    "\n",
    "The subtraction gives us a column like `streak_id`\n",
    "\n",
    "We do this in the following CTE:\n",
    "\n",
    "```sql\n",
    "WITH CTE AS\n",
    "(\n",
    "    SELECT *,\n",
    "    ROW_NUMBER() OVER(PARTITION BY player_id ORDER BY match_date) AS `all_id`,\n",
    "    ROW_NUMBER() OVER(PARTITION BY player_id, match_result ORDER BY match_date) AS `win_loss_id`\n",
    "    FROM\n",
    "    players_results\n",
    "    ORDER BY player_id, match_date\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This gives us result like:\n",
    "\n",
    "![](https://i.imgur.com/p3f5y35.png)\n",
    "\n",
    "Now we compute the `streak_id`\n",
    "\n",
    "```sql\n",
    "CTE2 AS\n",
    "(\n",
    "SELECT \n",
    "player_id,\n",
    "match_date,\n",
    "match_result,\n",
    "CAST(win_loss_id AS SIGNED) - CAST(all_id AS SIGNED) AS `streak_id`\n",
    "FROM CTE\n",
    ")\n",
    "```\n",
    "\n",
    "This gives us:\n",
    "\n",
    "![](https://i.imgur.com/6AY21Nd.png)\n",
    "\n",
    "Now for each streak_id we want to count no of times it has occured (note streak ids might get repeated for wins and losses and also for diff players)\n",
    "Now we only care about winning results, so we filter by wins\n",
    "\n",
    "```sql\n",
    "CTE3 AS(\n",
    "SELECT player_id,\n",
    "streak_id, COUNT(*) AS `win_streak_length`\n",
    "FROM CTE2\n",
    "WHERE match_result='W'\n",
    "GROUP BY player_id, streak_id\n",
    ")\n",
    "```\n",
    "\n",
    "![](https://i.imgur.com/ab5PQP7.png)\n",
    "\n",
    "Finally we can get the max of winning streak lengths per player id:\n",
    "\n",
    "```sql\n",
    "SELECT player_id, \n",
    "MAX(win_streak_length)\n",
    "FROM CTE3\n",
    "GROUP BY player_id\n",
    "```\n",
    "\n",
    "![](https://i.imgur.com/qGxVHje.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-1cbcb41f-c94e-4d5d-97b7-5f78cfddcaaf",
    "deepnote_cell_height": 927.375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### SQL Question 1\n",
    "\n",
    "> https://platform.stratascratch.com/coding/9899-percentage-of-total-spend?python&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n",
    "\n",
    "```\n",
    "--- INNER JOIN AS WE WANT ONLY CUSTOMERS WHO HAVE PLACED AN ORDER\n",
    "\n",
    "select first_name,order_details,\n",
    "total_order_cost/SUM(total_order_cost) OVER(PARTITION BY first_name) AS \"percentage of the order cost\"\n",
    "\n",
    "from orders o\n",
    "INNER JOIN customers c\n",
    "ON o.cust_id = c.id\n",
    "```\n",
    "\n",
    "#### SQL Question 2\n",
    "\n",
    "> https://platform.stratascratch.com/coding/2036-lowest-revenue-generated-restaurants?python&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n",
    "\n",
    "```\n",
    "--- Filter data to only use May 2020 records\n",
    "SELECT * FROM (\n",
    "SELECT \n",
    "    restaurant_id,\n",
    "    order_total,\n",
    "    NTILE(100) OVER (ORDER BY order_total ASC) AS percentile_value\n",
    "FROM (\n",
    "\n",
    "SELECT \n",
    "    --EXTRACT(MONTH FROM customer_placed_order_datetime) as order_month,\n",
    "    --EXTRACT(YEAR FROM customer_placed_order_datetime) as order_year,\n",
    "    restaurant_id, \n",
    "    SUM(order_total) AS order_total\n",
    "FROM doordash_delivery\n",
    "WHERE EXTRACT(MONTH FROM customer_placed_order_datetime) = 5 and EXTRACT(YEAR FROM customer_placed_order_datetime) = 2020\n",
    "GROUP BY restaurant_id\n",
    ") as sub1\n",
    ") as sub0\n",
    "WHERE percentile_value < 3\n",
    "ORDER BY 2 ASC\n",
    "```\n",
    "\n",
    "Instead of a subquery we can use a CTE, which is a bit easier to interpret\n",
    "Alse CTE creates temp tables, so we can re-use the query later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-33bfcba5-46be-40a8-bb9b-b069c2c878fa",
    "deepnote_cell_height": 522.921875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Leetcode 180. Consecutive Numbers\n",
    "\n",
    "> https://leetcode.com/problems/consecutive-numbers/\n",
    "\n",
    "```sql\n",
    "WITH CTE AS (\n",
    "SELECT\n",
    "num,\n",
    "LAG(num, 1) OVER() AS `prev_1`,\n",
    "LAG(num, 2) OVER() AS `prev_2`\n",
    "FROM \n",
    "Logs\n",
    "),\n",
    "CTE2 AS (\n",
    "    SELECT\n",
    "    CASE WHEN num = prev_1 and num = prev_2 THEN num\n",
    "    ELSE NULL END AS `ConsecutiveNums`\n",
    "    FROM CTE\n",
    ")\n",
    "\n",
    "SELECT DISTINCT ConsecutiveNums FROM CTE2\n",
    "WHERE ConsecutiveNums IS NOT NULL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-1686b787-49d0-4892-a03e-1be85c32029d",
    "deepnote_cell_height": 813.9375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Leetcode 262. Trips and Users\n",
    "\n",
    "> https://leetcode.com/problems/trips-and-users/\n",
    "\n",
    "```sql\n",
    "\n",
    "# Write your MySQL query statement below\n",
    "WITH CTE AS (\n",
    "SELECT request_at, status, u.banned AS user_banned, u1.banned AS driver_banned\n",
    "FROM Trips t\n",
    "INNER JOIN Users u\n",
    "ON t.client_id = u.users_id\n",
    "INNER JOIN Users u1 \n",
    "ON t.driver_id = u1.users_id\n",
    "    \n",
    "WHERE u.banned = \"No\" and u1.banned = \"No\" and request_at BETWEEN \"2013-10-01\" and \"2013-10-03\"\n",
    "),\n",
    "\n",
    "CTE2 AS\n",
    "(\n",
    "SELECT request_at,\n",
    "CASE WHEN status LIKE \"cancelled%\" THEN 1\n",
    "ELSE 0 END AS new_status\n",
    "FROM CTE\n",
    "),\n",
    "CTE3 AS \n",
    "(\n",
    "SELECT \n",
    "    request_at AS Day,\n",
    "    ROUND(SUM(new_status)/COUNT(new_status), 2) AS `Cancellation Rate`\n",
    "    FROM CTE2\n",
    "    GROUP BY request_at\n",
    ")\n",
    "\n",
    "SELECT * FROM CTE3\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0ea44f6b-e8d4-49f2-8459-5ba115252950",
    "deepnote_cell_height": 1019.625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Question: Monthly Percentage Difference\n",
    "\n",
    "> https://platform.stratascratch.com/coding/10319-monthly-percentage-difference?python=\n",
    "\n",
    "---\n",
    "\n",
    "### Soln\n",
    "\n",
    "```sql\n",
    "## get SUM(revenue) per year, month\n",
    "WITH CTE AS (\n",
    "select YEAR(created_at) AS `year`, MONTH(created_at) AS `month`, SUM(value) AS `monthly_revenue`\n",
    "from sf_transactions\n",
    "GROUP BY year, month\n",
    "ORDER BY year, month\n",
    "),\n",
    "\n",
    "### get prev month revenue\n",
    "CTE2 AS (\n",
    "SELECT year, month, monthly_revenue,\n",
    "LAG(monthly_revenue) OVER() AS `prev_revenue`\n",
    "FROM CTE\n",
    "),\n",
    "\n",
    "### get % change in revenue\n",
    "CTE3 AS (\n",
    "SELECT year, month, monthly_revenue,prev_revenue,\n",
    "ROUND((monthly_revenue-prev_revenue)*100/prev_revenue,2) AS `perc_change`\n",
    "FROM CTE2\n",
    "),\n",
    "\n",
    "### Zero pad month\n",
    "CTE4 AS (\n",
    "SELECT \n",
    "year, month, monthly_revenue,prev_revenue,perc_change,\n",
    "CASE WHEN month < 10 THEN CONCAT(\"0\", month)\n",
    "ELSE month \n",
    "END AS `zp_month`\n",
    "FROM CTE3\n",
    ")\n",
    "\n",
    "### final result\n",
    "SELECT\n",
    "CONCAT(year, \"-\", zp_month),\n",
    "perc_change\n",
    "FROM CTE4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d4d56c58-bb61-4a35-b78c-e14a22934138",
    "deepnote_cell_height": 456.75,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Activity Rank\n",
    "\n",
    "> https://platform.stratascratch.com/coding/10351-activity-rank?python=\n",
    "---\n",
    "\n",
    "\n",
    "```sql\n",
    "### cte to compute num of emails sent by each user\n",
    "### and order by reqd specifications\n",
    "### cannot use rank() as duplicates should have diff ranks\n",
    "WITH CTE AS \n",
    "(\n",
    "select from_user, COUNT(from_user) AS `total_sent`\n",
    "FROM google_gmail_emails\n",
    "GROUP BY from_user\n",
    "ORDER BY COUNT(from_user) DESC, from_user ASC\n",
    ")\n",
    "\n",
    "SELECT from_user, total_sent,\n",
    "ROW_NUMBER() OVER() AS `custom_rank`\n",
    "FROM CTE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17a08a47-073e-41f6-9915-880d0fb6994c",
    "deepnote_cell_height": 966.484375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Popularity Percentage\n",
    "\n",
    "> https://platform.stratascratch.com/coding/10284-popularity-percentage?python=&utm_source=youtube&utm_medium=click&utm_campaign=YT+description+link\n",
    "\n",
    "> https://www.youtube.com/watch?v=_gy1o9UH2dQ\n",
    "\n",
    "---\n",
    "\n",
    "### Soln\n",
    "\n",
    "```sql\n",
    "### get each user1 and count(user2) \n",
    "### get each user2 and count(user1)\n",
    "### UNION these to get each user and total_friends\n",
    "\n",
    "WITH CTE AS (\n",
    "select user1, COUNT(user2) AS `total_friends`\n",
    "FROM facebook_friends\n",
    "GROUP BY user1\n",
    "\n",
    "UNION\n",
    "\n",
    "select user2, COUNT(user1) AS `total_friends`\n",
    "FROM facebook_friends\n",
    "GROUP BY user2\n",
    "),\n",
    "\n",
    "### get each user and take a sum of total_friends in case of repeated user ids\n",
    "\n",
    "CTE2 AS (\n",
    "SELECT user1 AS user, \n",
    "SUM(total_friends) AS `total_friends`\n",
    "FROM CTE\n",
    "GROUP BY user1\n",
    ")\n",
    "\n",
    "### compute % - get total no of users in the table from a subquery\n",
    "\n",
    "SELECT user,\n",
    "total_friends*100/(SELECT COUNT(*) FROM CTE2)\n",
    "FROM CTE2\n",
    "\n",
    "ORDER BY user\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-3a7f2aa0-b931-481b-93f6-0d3b646e8487",
    "deepnote_cell_height": 1457.640625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## SQL Moving Averages\n",
    "\n",
    "> https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/\n",
    "\n",
    "> https://learnsql.com/blog/moving-average-in-sql/\n",
    "---\n",
    "\n",
    "The moving average is a time series technique for analyzing and determining trends in data. Sometimes called rolling means, rolling averages, or running averages, they are calculated as the mean of the current and a specified number of immediately preceding values for each point in time. The main idea is to examine how these averages behave over time instead of examining the behavior of the original or raw data points.\n",
    "\n",
    "![](https://learnsql.com/blog/moving-average-in-sql/Image-2.png)\n",
    "\n",
    "```sql\n",
    "select *,\n",
    "  avg(Price) OVER(ORDER BY Date\n",
    "     ROWS BETWEEN 2 PRECEDING AND CURRENT ROW )\n",
    "     as moving_average\n",
    "from stock_price;\n",
    "```\n",
    "\n",
    "- We use a window function, denoted with an OVER clause. As explained earlier, the rows are not collapsed, and each row has its own window over which a calculation is performed.\n",
    "\n",
    "- The size of the window in our example is 3. For each given row, we take the row itself and the two previous rows, and we calculate the average price from those three rows. This is denoted by the `ROW` keyword in the statement: `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW`. This statement says that, for each row in the table, something is calculated as an aggregation of the current and the previous two rows. This means that the moving average for each row is calculated as the mean price from the given day and the two previous days.\n",
    "\n",
    "- We have a different window frame for each day. Below, you can see an illustration of the window frame used for the row corresponding to January 9 (in green) and the window frame used for the row corresponding to June 27 (in blue):\n",
    "\n",
    "![](https://learnsql.com/blog/moving-average-in-sql/Image-5.png)\n",
    "\n",
    "- It is important that the data __not have any gaps in dates__. For each day, we need to calculate the average of the prices from that day and the two previous days. If there are missing dates in the data, this analysis will not make sense.\n",
    "\n",
    "- The `ORDER BY` keyword inside the `OVER` clause defines the order of the rows over which the moving average should be calculated. In our example, the rows are first sorted by the date column, then the window frame is defined, and the calculation is performed.\n",
    "\n",
    "- For this example, we do not use the `PARTITION BY` keyword in the `OVER` clause. `PARTITION BY` groups rows into logical chunks by some category, but we are not grouping rows that way here. In effect, our whole data set is just one large partition. \n",
    "\n",
    "\n",
    "```sql\n",
    "\n",
    "select *,\n",
    "  avg(Price) OVER(ORDER BY Date\n",
    "      ROWS BETWEEN 1 PRECEDING AND CURRENT ROW )\n",
    "     as 2day_moving_average,\n",
    "  avg(Price) OVER(ORDER BY Date\n",
    "      ROWS BETWEEN 29 PRECEDING AND CURRENT ROW )\n",
    "      as 30day_moving_average\n",
    "from stock_price;\n",
    "\n",
    "```\n",
    "\n",
    "![](https://learnsql.com/blog/moving-average-in-sql/3.png)\n",
    "\n",
    "Now let's practice a little more to recap what we have learned so far. COVID-19 is still very real in our lives, so we will calculate the seven-day moving average for the total number of confirmed cases by country. The number of confirmed cases for each day for each country is stored in a table named confirmed_covid, in the column confirmed_day:\n",
    "\n",
    "```sql\n",
    "SELECT *,\n",
    "      avg(confirmed_day) OVER(\n",
    "          PARTITION BY country\n",
    "          ORDER BY date\n",
    "          ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n",
    "          AS 7day_moving_average\n",
    "FROM confirmed_covid;\n",
    "```\n",
    "\n",
    "You may have noticed this code looks just like the one we wrote for the stock price moving average. The one key difference is that we have partitions here?—?each country is a partition, because we are calculating the seven-day averages separately for each country. Otherwise, only the table and the column names are different. You can easily adapt this code to any other business scenario.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ca3221a-a0a7-4fd1-812b-19d9646fb622",
    "deepnote_cell_height": 1466.234375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Meta SQL question \n",
    "\n",
    "> https://towardsdatascience.com/the-facebook-data-scientist-interview-38556739e872\n",
    "---\n",
    "\n",
    "Given two tables. One an attendance log for every student in a school district and the other a summary table with demographics for each student in the district.\n",
    "\n",
    "```\n",
    "attendance_events : date | student_id | attendance\n",
    "\n",
    "all_students : student_id | school_id | grade_level | date_of_birth | hometown\n",
    "```\n",
    "\n",
    "__What percent of students attend school on their birthday?__\n",
    "\n",
    "\n",
    "```sql\n",
    "-- step 1: join the 2 tables\n",
    "WITH cte AS\n",
    "(\n",
    "\tSELECT date, a.student_id, attendance, date_of_birth \n",
    "\t\tFROM attendance a\n",
    "\t\tINNER JOIN students s\n",
    "\t\tON a.student_id = b.student_id\n",
    "),\n",
    "-- step2: get num students attending on bday\n",
    "cte2 AS (\n",
    "\tSELECT COUNT(DISTINCT student_id) AS num_students_bday\n",
    "  FROM cte \n",
    "\tWHERE date = date_of_birth\n",
    "),\n",
    "\n",
    "--- ratio = num students / students attending on bday\n",
    "SELECT (SELECT num_students_bday FROM cte2)*100/COUNT(DISTINCT student_id) AS perc_bday\n",
    "FROM cte\n",
    "```\n",
    "\n",
    "__Which grade level had the largest drop in attendance between yesterday and today?__\n",
    "\n",
    "```sql\n",
    "-- step 1: join the 2 tables and compute the total attendance for each day and grade\n",
    "WITH cte AS\n",
    "(\n",
    "\tSELECT date, grade_level, SUM(attendance) AS attendance \n",
    "\tFROM attendance a\n",
    "\tINNER JOIN students s\n",
    "\tON a.student_id = b.student_id\n",
    "\tGROUP BY date, grade_level\n",
    "),\n",
    "-- step 2: get prev day attendance by grade level ordered by date\n",
    "cte2 AS\n",
    "(\n",
    "\tSELECT date, grade_level, attendance,\n",
    "\tLAG(attendance) OVER (PARTITION BY grade_level ORDER BY date) AS prev_day_attendance\n",
    "\tFROM cte\n",
    "),\n",
    "-- step 3: compute attendance drop for current day\n",
    "cte3 AS\n",
    "(\n",
    "\tSELECT date, grade_level, attendance, prev_day_attendance,\n",
    "\tprev_day_attendance - attendance AS attendance_drop\n",
    "\tFROM cte2\n",
    "\tWHERE date = (SELECT CURDATE())\n",
    "),\n",
    "-- step 4: get grade level with max attendance drop\n",
    "cte4 AS \n",
    "(\n",
    "\tSELECT grade_level\n",
    "\tFROM cte3\n",
    "\tWHERE attendance_drop = (SELECT MAX(attendance_drop) FROM cte3)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab282c7c-6dff-4ad2-9719-d7d4e1e03611",
    "deepnote_cell_height": 851.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Goodwater Analytics Question\n",
    "\n",
    "> https://www.youtube.com/watch?v=FC5X09o85xE\n",
    "\n",
    "---\n",
    "\n",
    "![](https://i.imgur.com/oSsFImr.png)\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/JkjgDUx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "252c597a-e42d-4398-8698-785fe657a4b3",
    "deepnote_cell_height": 1206.53125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "```sql\n",
    "--- get all downloads from ios and android and UNION them\n",
    "\n",
    "WITH all_product_downloads AS\n",
    "(\n",
    "    SELECT product_id, ios_app_id AS app_id, date, downloads\n",
    "    FROM Products p\n",
    "    INNER JOIN Downloads d\n",
    "    ON p.ios_app_id = d.app_id\n",
    "\n",
    "    UNION\n",
    "\n",
    "    SELECT product_id, android_app_id AS app_id, date, downloads\n",
    "    FROM Products p\n",
    "    INNER JOIN Downloads d\n",
    "    ON p.android_app_id = d.app_id\n",
    "),\n",
    "--- aggregate downloads at a product - date level\n",
    "product_date_data AS (\n",
    "    SELECT product_id, date, SUM(downloads) AS total_downloads\n",
    "    FROM all_product_downloads\n",
    "    GROUP BY product_id, date\n",
    "\n",
    ")\n",
    "```\n",
    "\n",
    "Now we have product - date level downloads data\n",
    "\n",
    "Imagine at the next step we calculate 28_day_rolling_downloads like:\n",
    "\n",
    "`product_id | date | 28_day_rolling_avg`\n",
    "\n",
    "```sql\n",
    "28_day_rolling_downloads AS\n",
    "(\n",
    "SELECT product_id, date,\n",
    "AVG(downloads) OVER(PARTITION BY product_id ORDER BY date\n",
    "ROWS BETWEEN 27 PRECEEDING AND CURRENT ROW\n",
    ") AS 28_day_rolling_avg\n",
    "FROM product_date_data\n",
    "\n",
    "ORDER BY product, date\n",
    ")\n",
    "```\n",
    "\n",
    "```sql\n",
    "\n",
    "--- get prev year rolling avg data\n",
    "\n",
    "yoy_data AS (\n",
    "SELECT product_id, date, 28_day_rolling_avg\n",
    "LAG(28_day_rolling_avg, 365) OVER(PARTITION BY product_id ORDER BY date) AS prev_year_avg\n",
    "FROM rolling_cte\n",
    ")\n",
    "\n",
    "SELECT product, date,\n",
    "(28_day_rolling_avg - prev_year_avg)*100/prev_year_avg as YoY_growth\n",
    "FROM rolling_cte\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7b705951-93cf-427a-b11d-35754fe57417",
    "deepnote_cell_height": 494.984375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Alternative: Imagine u cant compute rolling avg directly - how will u do it\n",
    "\n",
    "product_id | date | total_downloads\n",
    "\n",
    "The way to do this is by using a self join\n",
    "We can specify AND conditions while joining which tells sql\n",
    "to keep rows where the datediff is bw 0 and 28\n",
    "\n",
    "```sql\n",
    "\n",
    "28_day_rolling_downloads AS(\n",
    "SELECT\n",
    "p1.product_id, p1.date, AVG(p2.total_downloads) AS 28_day_rolling_avg\n",
    "FROM product_date_data p1\n",
    "INNER JOIN product_date_data p2\n",
    "ON p1.product_id = p2.product_id\n",
    "AND DATEDIFF(p1.date, p2.date) <= 28\n",
    "AND DATEDIFF(p1.date, p2.date) >= 0\n",
    "\n",
    "GROUP BY p1.product_id, p1.date\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "SELECT d1.product_id, d1.date, (d2.28_day_rolling_avg\n",
    "FROM 28_day_rolling_downloads d1\n",
    "INNER JOIN 28_day_rolling_downloads d2\n",
    "ON d1.product_id = d2.product_id\n",
    "AND DATEDIFF(d1.date, d2.date) = 365\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b9623962771a4f04934a987c9ccc9c3e",
    "deepnote_cell_height": 1200.09375,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "a7a3883e-865f-47ed-8be0-6b6e2a6517cb",
    "tags": []
   },
   "source": [
    "### Advanced Facebook Data Science SQL interview question\n",
    "\n",
    "> https://www.youtube.com/watch?v=PlpUo6bHsBQ\n",
    "\n",
    "#### Soln:\n",
    "\n",
    "```sql\n",
    "-- join the two tables and filter on dates\n",
    "WITH cte AS\n",
    "(\n",
    "select a.user_id, created_at, country, number_of_comments, MONTH(created_at) AS month\n",
    "from fb_comments_count a\n",
    "INNER JOIN fb_active_users b\n",
    "ON a.user_id = b.user_id\n",
    "WHERE created_at >= '2019-12-01' AND created_at <= '2020-01-31'\n",
    "),\n",
    "-- country - total comments for dec\n",
    "dec_data AS\n",
    "(\n",
    "SELECT \n",
    "country, SUM(number_of_comments) AS total_comments\n",
    "FROM cte\n",
    "WHERE month = 12\n",
    "GROUP BY country\n",
    "),\n",
    "-- country - total comments for jan\n",
    "jan_data AS \n",
    "(\n",
    "SELECT \n",
    "country, SUM(number_of_comments) AS total_comments\n",
    "FROM cte\n",
    "WHERE month = 1\n",
    "GROUP BY country\n",
    "),\n",
    "-- country rank for dec\n",
    "dec_rank AS\n",
    "(\n",
    "SELECT country,total_comments,\n",
    "DENSE_RANK() OVER(ORDER BY total_comments DESC) AS `rank`\n",
    "FROM dec_data\n",
    "),\n",
    "-- country rank for jan\n",
    "jan_rank AS\n",
    "(\n",
    "SELECT country,total_comments,\n",
    "DENSE_RANK() OVER(ORDER BY total_comments DESC) AS `rank`\n",
    "FROM jan_data\n",
    ")\n",
    "-- join the two\n",
    "SELECT a.country, a.rank AS dec_rank, b.rank AS jan_rank\n",
    "FROM dec_rank a\n",
    "INNER JOIN jan_rank b\n",
    "ON a.country = b.country\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5d6545a033734938af873e18b3fc83de",
    "deepnote_cell_height": 1009.515625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Oracle SQL Question\n",
    "\n",
    "> https://www.youtube.com/watch?v=iKRpjcfODoY\n",
    "\n",
    "> https://www.interviewquery.com/questions/always-excited-users\n",
    "\n",
    "---\n",
    "\n",
    "```sql\n",
    "--- first select unique users who have never been bored\n",
    "with excited_users AS (\n",
    "SELECT DISTINCT user_id\n",
    "FROM ad_impressions \n",
    "WHERE COUNT(DISTINCT impression_id) == 1 AND impression_id = 'excited'\n",
    "),\n",
    "--- select currently excited users\n",
    "current_excited AS (\n",
    "SELECT DISTINCT user_id\n",
    "FROM ad_impressions\n",
    "WHERE impression_id = 'excited' AND dt = (SELECT CURRDATE() FROM ad_impressions)\n",
    ")\n",
    "--- select current excited who have never been bored\n",
    "SELECT user_id FROM current_excited\n",
    "WHERE user_id IN (SELECT user_id FROM excited_users)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Forecasting\n",
    "\n",
    "> https://platform.stratascratch.com/coding/10313-naive-forecasting?code_type=1\n",
    "\n",
    "---\n",
    "\n",
    "```sql\n",
    "\n",
    "-- get the total_distance and total_cost per month\n",
    "WITH cte \n",
    "AS\n",
    "(\n",
    "    select DATE_PART('month', request_date) AS request_month,\n",
    "    SUM(distance_to_travel) AS total_distance,\n",
    "    SUM(monetary_cost) AS total_cost\n",
    "    from uber_request_logs\n",
    "    GROUP BY DATE_PART('month', request_date) \n",
    "),\n",
    "-- compute distance per dollar for each month\n",
    "cte2\n",
    "AS\n",
    "(\n",
    "    SELECT request_month,\n",
    "    total_distance/total_cost AS distance_per_dollar\n",
    "    FROM cte\n",
    "),\n",
    "-- get the prev months distance_per_dollar\n",
    "cte3\n",
    "AS\n",
    "(\n",
    "    SELECT request_month, distance_per_dollar AS actual,\n",
    "    LAG(distance_per_dollar, 1, 0)\n",
    "    OVER(ORDER BY request_month) AS forecast\n",
    "    FROM cte2\n",
    ")\n",
    "\n",
    "SELECT * FROM cte3\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Percentage Difference\n",
    "\n",
    "\n",
    "> https://platform.stratascratch.com/coding/10319-monthly-percentage-difference?code_type=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "WITH cte\n",
    "AS\n",
    "(\n",
    "    select DATE_PART('month', created_at) AS month,\n",
    "    SUM(value) AS monthly_revenue\n",
    "    FROM sf_transactions\n",
    "    GROUP BY DATE_PART('month', created_at)\n",
    "    ORDER BY month\n",
    "),\n",
    "\n",
    "cte2 AS \n",
    "(\n",
    "    SELECT month, monthly_revenue,\n",
    "    LAG(monthly_revenue)\n",
    "    OVER (ORDER BY month) AS prev_month_revenue\n",
    "    FROM cte\n",
    ")\n",
    "\n",
    "SELECT month,\n",
    "monthly_revenue, prev_month_revenue,\n",
    "(monthly_revenue-prev_month_revenue)*100/prev_month_revenue AS increase\n",
    "FROM cte2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c9f7b205-46e2-4f7d-8027-1722d788f5d8' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "3f480396-3090-4c9d-95d9-b94e0489f5a1",
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
